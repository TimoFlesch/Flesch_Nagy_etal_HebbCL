{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playground for hebbian learning \n",
    "a few sanity checks to gain deeper understanding of why SLA works the way it does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial.distance import squareform,pdist\n",
    "from scipy.stats import multivariate_normal\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen2Dgauss(x_mu=.0, y_mu=.0, xy_sigma=.1, n=20):\n",
    "    \"\"\"\n",
    "    generates two-dimensional gaussian blob\n",
    "    \"\"\"\n",
    "    xx,yy = np.meshgrid(np.linspace(0, 1, n),np.linspace(0, 1, n))\n",
    "    gausspdf = multivariate_normal([x_mu,y_mu],[[xy_sigma,0],[0,xy_sigma]])\n",
    "    x_in = np.empty(xx.shape + (2,))\n",
    "    x_in[:, :, 0] = xx; x_in[:, :, 1] = yy\n",
    "    return gausspdf.pdf(x_in)\n",
    "\n",
    "\n",
    "def make_blobs_block(context, do_shuffle, c_scaling=1):\n",
    "    \"\"\"\n",
    "    generates block of experiment\n",
    "    Input:\n",
    "      - task  : 'task_a' or 'task_b'\n",
    "      - do_shuffle: True or False, shuffles  values\n",
    "    \"\"\"\n",
    "    resolution = 5\n",
    "    n_units = resolution**2\n",
    "    l, b = np.meshgrid(np.linspace(0.2, .8, 5),np.linspace(0.2, .8, 5))\n",
    "    b = b.flatten()\n",
    "    l = l.flatten()\n",
    "    r_s, r_n = np.meshgrid(np.linspace(-2, 2, 5),np.linspace(-2, 2, 5))\n",
    "    r_s = r_s.flatten()\n",
    "    r_n = r_n.flatten()\n",
    "    val_l, val_b = np.meshgrid(np.linspace(1, 5, 5),np.linspace(1, 5, 5))\n",
    "    val_b = val_b.flatten()\n",
    "    val_l = val_l.flatten()\n",
    "\n",
    "    #plt.figure()\n",
    "    ii_sub = 1\n",
    "    blobs = np.empty((25,n_units))\n",
    "    for ii in range(0,25):\n",
    "        blob = gen2Dgauss(x_mu=b[ii], y_mu=l[ii],xy_sigma=0.08,n=resolution)\n",
    "        blob = blob/ np.max(blob)\n",
    "        ii_sub += 1\n",
    "        blobs[ii,:] = blob.flatten()\n",
    "    x = blobs\n",
    "    if context == 'task_a':\n",
    "        x1 = np.append(blobs,c_scaling * np.ones((blobs.shape[0],1)),axis=1)\n",
    "        x1 = np.append(x1,np.zeros((blobs.shape[0],1)),axis=1)\n",
    "        reward = r_n\n",
    "    elif context == 'task_b':\n",
    "        x1 = np.append(blobs,np.zeros((blobs.shape[0],1)),axis=1)\n",
    "        x1 = np.append(x1,c_scaling * np.ones((blobs.shape[0],1)),axis=1)\n",
    "        reward = r_s\n",
    "\n",
    "    feature_vals = np.vstack((val_b,val_l)).T\n",
    "    if do_shuffle:\n",
    "        ii_shuff = np.random.permutation(25)\n",
    "        x1 = x1[ii_shuff,:]\n",
    "        feature_vals = feature_vals[ii_shuff,:]\n",
    "        reward = reward[ii_shuff]\n",
    "    return x1, reward, feature_vals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_blobs_dataset(ctx_scaling=1, training_schedule='blocked',n_episodes=10, ctx_avg=True, ctx_avg_window=10, centering=True):\n",
    "    \"\"\"\n",
    "    makes dataset for experiment\n",
    "    \"\"\"\n",
    "    \n",
    "    random_state = np.random.randint(999)\n",
    "\n",
    "    x_test_a,y_test_a,f_test_a = make_blobs_block('task_a',0,ctx_scaling)\n",
    "    y_test_a = y_test_a[:,np.newaxis]\n",
    "    l_test_a = (y_test_a>0).astype('int')\n",
    "\n",
    "    x_test_b,y_test_b,f_test_b = make_blobs_block('task_b',0,ctx_scaling)\n",
    "    y_test_b = y_test_b[:,np.newaxis]\n",
    "    l_test_b = (y_test_b>0).astype('int')\n",
    "\n",
    "    x_in = np.concatenate((x_test_a,x_test_b),axis=0)\n",
    "    y_rew = np.concatenate((y_test_a,y_test_b), axis=0)\n",
    "    y_true = np.concatenate((l_test_a,l_test_b), axis=0)\n",
    "\n",
    "    # define datasets (duplicates for shuffling)\n",
    "    data = {}\n",
    "    data['x_test_a'] = x_test_a\n",
    "    data['y_test_a'] = y_test_a\n",
    "    data['l_test_a'] = l_test_a\n",
    "\n",
    "    data['x_test_b'] = x_test_b\n",
    "    data['y_test_b'] = y_test_b\n",
    "    data['l_test_b'] = l_test_b\n",
    "\n",
    "    data['x_all'] = x_in\n",
    "    data['y_all'] = y_rew\n",
    "    data['l_all'] = y_true\n",
    "\n",
    "    if training_schedule == 'interleaved':\n",
    "        data['x_train'] = np.vstack(tuple([shuffle(data['x_all'],random_state = i+random_state) for i in range(n_episodes)]))\n",
    "        data['y_train'] = np.vstack(tuple([shuffle(data['y_all'],random_state = i+random_state) for i in range(n_episodes)]))\n",
    "        data['l_train'] = np.vstack(tuple([shuffle(data['l_all'],random_state = i+random_state) for i in range(n_episodes)]))\n",
    "    elif training_schedule == 'blocked':\n",
    "        data['x_train'] = np.vstack((\n",
    "            np.vstack(tuple([shuffle(data['x_test_a'],random_state = i+random_state) for i in range(n_episodes)])),\n",
    "            np.vstack(tuple([shuffle(data['x_test_b'],random_state = i+random_state) for i in range(n_episodes)]))))\n",
    "        data['y_train'] = np.vstack((\n",
    "            np.vstack(tuple([shuffle(data['y_test_a'],random_state = i+random_state) for i in range(n_episodes)])),\n",
    "            np.vstack(tuple([shuffle(data['y_test_b'],random_state = i+random_state) for i in range(n_episodes)]))))\n",
    "        data['l_train'] = np.vstack((\n",
    "            np.vstack(tuple([shuffle(data['l_test_a'],random_state = i+random_state) for i in range(n_episodes)])),\n",
    "            np.vstack(tuple([shuffle(data['l_test_b'],random_state = i+random_state) for i in range(n_episodes)]))))    \n",
    "    else:\n",
    "        print('Unknown training schedule parameter')\n",
    "        \n",
    "\n",
    "    if ctx_avg and ctx_avg_window>0:    \n",
    "        data['x_train'][:,-2] = pd.Series(data['x_train'][:,-2]).rolling(window=ctx_avg_window, min_periods=1).mean()\n",
    "        data['x_train'][:,-1] = pd.Series(data['x_train'][:,-1]).rolling(window=ctx_avg_window, min_periods=1).mean()\n",
    "\n",
    "    if centering == True:\n",
    "        sc = StandardScaler(with_std=False)\n",
    "        data['x_train'] = sc.fit_transform(data['x_train'])\n",
    "        data['x_test_a'] = sc.transform(data['x_test_a'])\n",
    "        data['x_test_b'] = sc.transform(data['x_test_b'])\n",
    "        data['x_all'] = sc.transform(data['x_all'])  \n",
    "    return data\n",
    "\n",
    "\n",
    "def biplot(score,coeff,pcax,pcay,labels=None):\n",
    "    '''\n",
    "    pyplot doesn't support biplots as matlab does. got this script from\n",
    "        https://sukhbinder.wordpress.com/2015/08/05/biplot-with-python/\n",
    "    \n",
    "    USAGE: biplot(score,pca.components_,1,2,labels=categories)\n",
    "    '''\n",
    "    pca1=pcax-1\n",
    "    pca2=pcay-1\n",
    "    xs = score[:,pca1]\n",
    "    ys = score[:,pca2]\n",
    "    n=score.shape[1]\n",
    "    scalex = 1.0/(xs.max()- xs.min())\n",
    "    scaley = 1.0/(ys.max()- ys.min())\n",
    "    plt.scatter(xs*scalex,ys*scaley)\n",
    "    print(n)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5) \n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, labels[i], color='g', ha='center', va='center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(pcax))\n",
    "    plt.ylabel(\"PC{}\".format(pcay))\n",
    "    plt.grid()\n",
    "    \n",
    "def plot_results():\n",
    "    \n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in delta_ws])\n",
    "    plt.title('norm of ' + r'$\\Delta w$')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in ws])\n",
    "    plt.title('norm of ' + r'$w$')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.subplot(2,2,3)\n",
    "    a = plt.plot([wi[-1] for wi in ws],color='blue')\n",
    "    b = plt.plot([wi[-2] for wi in ws],color='orange')\n",
    "    plt.legend(['task a', 'task b'])\n",
    "    plt.title('context units')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('weight')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(np.asarray(ws).T)\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('input')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.title('all weights')\n",
    "    plt.suptitle(r'$\\eta = $' +f'{eta}' + r'  $\\sigma_{init} = $' + f'{sigma}',fontweight='normal',fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_initsign_results(init_weights=[1,-1]):\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in delta_ws])\n",
    "    plt.title('norm of ' + r'$\\Delta w$' )\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in ws])\n",
    "    plt.title('norm of ' + r'$w$')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.subplot(1,3,3)\n",
    "    a = plt.plot([wi[-1] for wi in ws],color='blue')\n",
    "    b = plt.plot([wi[-2] for wi in ws],color='orange')\n",
    "    plt.legend(['task a', 'task b'])\n",
    "    plt.title('context weights')\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('weight')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    plt.suptitle(r'$\\eta = $' +f'{eta}' + r'  $w_{init} = $' + f'{init_weights}',fontweight='normal',fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_ghasla_results():\n",
    "    \n",
    "    #stats:\n",
    "    plt.figure(figsize=(10,6),dpi=300)\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in delta_ws])\n",
    "    plt.title('norm of ' + r'$\\Delta W$' ,fontsize=8)\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    \n",
    "    plt.subplot(2,3,2)\n",
    "    plt.plot([np.linalg.norm(wi) for wi in ws])\n",
    "    plt.title('norm of ' + r'$W$' ,fontsize=8)\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('norm')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    \n",
    "    plt.subplot(2,3,3)    \n",
    "    a = plt.plot([wi[-2] for wi in ws],color='blue')\n",
    "    b = plt.plot([wi[-1] for wi in ws],color='orange')\n",
    "    plt.legend([a[0],b[0]],['task a', 'task b'],frameon=False)\n",
    "    plt.title('context to hidden weights' ,fontsize=8)\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('weight')\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    \n",
    "    plt.subplot(2,3,4)\n",
    "    plt.plot([np.corrcoef(wi[-1,:],wi[-2,:])[0,1] for wi in ws])\n",
    "    plt.title('correlation between ctx weight vectors' ,fontsize=8)\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel(\"pearson's r\")\n",
    "    plt.plot([n_trials//2, n_trials//2],plt.ylim(),'k:')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,3,5)\n",
    "    ta = []\n",
    "    tb = []\n",
    "    for W in ws:\n",
    "        \n",
    "        if W.shape[0]==2:\n",
    "            ya = np.maximum((data['x_test_a'][:,-2:]@W),0).mean(0)\n",
    "            yb = np.maximum((data['x_test_b'][:,-2:]@W),0).mean(0)\n",
    "        else:\n",
    "            ya = np.maximum((data['x_test_a']@W),0).mean(0)\n",
    "            yb = np.maximum((data['x_test_b']@W),0).mean(0)\n",
    "        ta.append(np.mean((ya>0)&(yb==0)))\n",
    "        tb.append(np.mean((ya==0)&(yb>0)))    \n",
    "    plt.plot(ta,color='blue')\n",
    "    plt.plot(tb,color='orange')\n",
    "    plt.title('proportion of task-specific units' ,fontsize=8)\n",
    "    plt.yticks(ticks=np.arange(0,1.1,0.1),labels=(np.arange(0,1.1,0.1)*100).astype('int'))\n",
    "    plt.xlabel('iter')\n",
    "    plt.ylabel('percentage')\n",
    "    plt.legend(['task a','task b'],frameon=False)\n",
    "    plt.suptitle('Learning Dynamics, ' + r'$\\eta = $' +f'{eta}' + r'  $\\sigma_{init} = $' + f'{sigma}',fontweight='normal',fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # connectivity matrix:\n",
    "    plt.figure(figsize=(8,3),dpi=300)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(ws[0])\n",
    "    plt.ylabel('input unit')\n",
    "    plt.xlabel('hidden unit / PC')\n",
    "    plt.title('Initial Connectivity Matrix',fontsize=8)\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(ws[-1])\n",
    "    plt.ylabel('input unit')\n",
    "    plt.xlabel('hidden unit / PC')\n",
    "    plt.title('Endpoint Connectivity Matrix',fontsize=8)\n",
    "    plt.suptitle('Weights',fontweight='normal',fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # receptive fields:\n",
    "#     plt.figure(figsize=(25,5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_blobs_dataset(n_episodes=1,ctx_scaling=5,ctx_avg=False,centering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data['x_all'][:10,:])\n",
    "plt.ylabel('stimulus')\n",
    "plt.xlabel('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x_train'][:10,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs = plt.subplots(5,5,figsize=(10,10))\n",
    "axs = axs.ravel()\n",
    "for ii,ax in enumerate(axs):\n",
    "    ax.imshow(data['x_all'][ii,:25].reshape((5,5)))\n",
    "_ = f.suptitle('Stimulus Space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on inputs \n",
    "Sanity check, should recover context as largest axis of variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from copy import deepcopy\n",
    "pca = PCA(n_components=27)\n",
    "data = make_blobs_dataset(n_episodes=1,ctx_scaling=5,ctx_avg=False,centering=True)\n",
    "\n",
    "\n",
    "pca.fit(data['x_train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings.shape\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data['x_train']@pca.components_.T\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c,t,m = np.meshgrid(np.arange(1,6),np.arange(1,3),np.arange(1,6))\n",
    "# c,m,t = c.flatten(), m.flatten(), t.flatten()\n",
    "# labels = ['c'+str(ic)+'m'+str(im)+'_t'+str(it) for ic,im,it in zip(c,m,t)]\n",
    "# labels_t = ['t'+str(it) for it in t]\n",
    "labels = ['']*25 + ['context1'] + ['context2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "score = scores\n",
    "coeff = pca.components_.T\n",
    "pcax = 1\n",
    "pcay =2\n",
    "\n",
    "plt.figure(figsize=(3,3),dpi=300)\n",
    "\n",
    "pca1=pcax-1\n",
    "pca2=pcay-1\n",
    "xs = score[:,pca1]\n",
    "ys = score[:,pca2]\n",
    "n=score.shape[1]\n",
    "scalex = 1.0/(xs.max()- xs.min())\n",
    "scaley = 1.0/(ys.max()- ys.min())\n",
    "plt.scatter(xs*scalex,ys*scaley)\n",
    "print(n)\n",
    "for i in range(n):\n",
    "    plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5, head_width=0.05) \n",
    "    if labels is None:\n",
    "        plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, labels[i], color='g', ha='center', va='center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xticks(np.arange(-1,1.1,0.5))\n",
    "plt.yticks(np.arange(-1,1.1,0.5))\n",
    "plt.xlabel(\"PC{}\".format(pcax))\n",
    "plt.ylabel(\"PC{}\".format(pcay))\n",
    "# plt.grid()\n",
    "sns.despine()\n",
    "plt.title('Biplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1) \n",
    "plt.bar(np.arange(len(pca.explained_variance_)),pca.explained_variance_/np.sum(pca.explained_variance_))\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='Principal Component',ylabel='Explained Variance',title='Explained Variance per PC')\n",
    "ax.set_xticks(np.arange(0,27,5))\n",
    "ax.set_xticklabels(np.arange(1,27,5))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.cumsum(pca.explained_variance_)/np.sum(pca.explained_variance_),'o-')\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel='Principal Component',ylabel='Cumulative Explained Var',title='Cumulative Explained Variance')\n",
    "ax.set_xticks(np.arange(0,27,5))\n",
    "ax.set_xticklabels(np.arange(1,27,5))\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "# Visualise learned components, interpret as connectivity matrix\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(pca.components_[:10,:])\n",
    "plt.title('Learned PCs (\"Connectivity Matrix\")')\n",
    "plt.xlabel('input nodes')\n",
    "plt.ylabel(\"'hidden nodes' / PCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are ctx vectors orthogonal:\n",
    "print(np.isclose(pca.components_[:,-1].T@pca.components_[:,-2],0))\n",
    "# what are the ctx to hidden weights for the first pc\n",
    "print(pca.components_[1,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned principal component weights\n",
    "a = plt.plot(pca.components_[:10,-2],'-',color='orange')\n",
    "b = plt.plot(pca.components_[:10,-1],'-',color='blue')\n",
    "plt.legend(['task a','task b'])\n",
    "plt.xlabel('principal component')\n",
    "plt.ylabel('ctx weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hebbian Learning on inputs\n",
    "This should arrive at the same solution as PCA, but from blocked data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hebbian Learning with a single hidden unit\n",
    "Let's verify that a single unit strengthens the connections between the context units and the hidden unit:  \n",
    "$$ \\Delta \\textbf{w} = \\eta y\\textbf{x} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-2\n",
    "sigma = 1e-8\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "w = np.random.randn(27)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(w))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = w.T@x    \n",
    "    dw = eta*y*x    \n",
    "    w += dw\n",
    "    delta_ws.append(dw)\n",
    "    ws.append(deepcopy(w))\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but with larger learning rate\n",
    "eta = 1e-1\n",
    "sigma = 1e-8\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False)\n",
    "w = np.random.randn(27)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(w))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = w.T@x    \n",
    "    dw = eta*y*x    \n",
    "    w += dw\n",
    "    delta_ws.append(dw)\n",
    "    ws.append(deepcopy(w))\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger weights at init\n",
    "eta = 1e-1\n",
    "sigma = 1e-2\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False)\n",
    "w = np.random.randn(27)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(w))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = w.T@x    \n",
    "    dw = eta*y*x    \n",
    "    w += dw\n",
    "    delta_ws.append(dw)\n",
    "    ws.append(deepcopy(w))\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oja's Rule with a single hidden unit\n",
    "The result above suggests that connections between the two context units (input 26 and 27 in subplot 2,2) get anticorrelated as training progresses. Unless the learning rate is small, weights grow really large. To prevent weights from exploding, we perform updates with Oja's rule, which adds weight decay:\n",
    "$$ \\Delta \\textbf{w} = \\eta (y\\textbf{x} - y^2\\textbf{w}) = \\eta y(\\textbf{x} - y\\textbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now with Oja's rule\n",
    "eta = 4e-2\n",
    "sigma = 1e-2\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False)\n",
    "w = np.random.randn(27)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(w))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = w.T@x    \n",
    "    dw = eta*y*(x-y*w.T)    \n",
    "    w += dw\n",
    "    delta_ws.append(dw)\n",
    "    ws.append(deepcopy(w))\n",
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do the results depend on the sign of the weights at initialisation?\n",
    "Potential problem: What if weights have to be anti-correlated from the start and Hebb only exploits this pre-existing structure?  Let's leave the input nodes aside and focus only on the two context nodes. How does convergence depend on the initial values for the context weights?   \n",
    "Intuition: We need to break symmetry. That is, weights should have different magnitude, otherwise they cancel each other out due to the (1,-1) inputs. But the sign may not matter.   \n",
    "Setup:\n",
    "Inputs are c*(1,-1) for task 1, followed by c*(-1,1) for task 2, where c is a scaling factor.\n",
    "Weights are again updated with Oja's rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO explore weight symmetry:\n",
    "# how does this depend on initialisation?\n",
    "# compare (w_c1,w_c2): (1,1), (-1,-1), (1,-1), (-1,1), (.5,1), (1,.5), (-.5,-1), (-1,-.5)\n",
    "# -> relate to symmetry breaking property of large weight initialisation\n",
    "\n",
    "w_init = np.array([[1,1],[-1,-1],[1,-1],[-1,1],[1,.5],[.5,1],[-1,-.5],[-.5,-1]]).astype('float')*1e-1\n",
    "eta = 5e-2\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "for wi in w_init:\n",
    "    w = deepcopy(wi)\n",
    "    delta_ws = []\n",
    "    ws = []\n",
    "    delta_ws.append(0)\n",
    "    ws.append(deepcopy(w))\n",
    "    X = data['x_train'][:,-2:]\n",
    "    for x in X:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        delta_ws.append(dw)\n",
    "        ws.append(deepcopy(w))\n",
    "    plot_initsign_results(wi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do the results depend on weight scale\n",
    "Larger weights -> more symmetry breaking -> earlier factorisation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_init = np.array([[10**(-i),-10**(-i)] for i in range(1,12,2)])\n",
    "eta = 5e-2\n",
    "n_episodes = 10\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "for wi in w_init:\n",
    "    w = deepcopy(wi)\n",
    "    delta_ws = []\n",
    "    ws = []\n",
    "    delta_ws.append(0)\n",
    "    ws.append(deepcopy(w))\n",
    "    X = data['x_train'][:,-2:]\n",
    "    for x in X:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        delta_ws.append(dw)\n",
    "        ws.append(deepcopy(w))\n",
    "    plot_initsign_results(wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do the results depend on centering\n",
    "one-hot vs centered.. (Note: with ctx_scaling=2 I get anti-correlated but very small weights for small init weights, but leave this aside for now. would be too small to turn hidden units on/off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO explore weight symmetry:\n",
    "# how does this depend on initialisation?\n",
    "# compare (w_c1,w_c2): (1,1), (-1,-1), (1,-1), (-1,1), (.5,1), (1,.5), (-.5,-1), (-1,-.5)\n",
    "# -> relate to symmetry breaking property of large weight initialisation\n",
    "\n",
    "w_init = np.array([[1,1],[-1,-1],[1,-1],[-1,1],[1,.5],[.5,1],[-1,-.5],[-.5,-1]]).astype('float')*1e-4\n",
    "eta = 1e-1\n",
    "n_episodes = 5\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=2,training_schedule='blocked',ctx_avg=False,centering=False)\n",
    "for wi in w_init:\n",
    "    w = deepcopy(wi)\n",
    "    delta_ws = []\n",
    "    ws = []\n",
    "    delta_ws.append(0)\n",
    "    ws.append(deepcopy(w))\n",
    "    X = data['x_train'][:,-2:]\n",
    "    for x in X:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        delta_ws.append(dw)\n",
    "        ws.append(deepcopy(w))\n",
    "    plot_initsign_results(wi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_init = np.array([[10**(-i),-10**(-i)] for i in range(1,6,1)])\n",
    "eta = 3e-2\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False,centering=False)\n",
    "for wi in w_init:\n",
    "    w = deepcopy(wi)\n",
    "    delta_ws = []\n",
    "    ws = []\n",
    "    delta_ws.append(0)\n",
    "    ws.append(deepcopy(w))\n",
    "    X = data['x_train'][:,-2:]\n",
    "    for x in X:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        delta_ws.append(dw)\n",
    "        ws.append(deepcopy(w))\n",
    "    plot_initsign_results(wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " what happens when we reduce the ctx scaling? (note: this no longer works when I train on the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.array([[-10**(-i),10**(-i)] for i in range(1,6,1)])\n",
    "eta = 3e-2\n",
    "n_episodes = 1\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=3,training_schedule='blocked',ctx_avg=False,centering=False)\n",
    "for wi in w_init:\n",
    "    w = deepcopy(wi)\n",
    "    delta_ws = []\n",
    "    ws = []\n",
    "    delta_ws.append(0)\n",
    "    ws.append(deepcopy(w))\n",
    "    X = data['x_train'][:,-2:]\n",
    "    for x in X:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        delta_ws.append(dw)\n",
    "        ws.append(deepcopy(w))\n",
    "    plot_initsign_results(wi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalised Hebbian Learning\n",
    "So far, we've focused on learning the associations between context units and a single hidden unit. However, there are more directions of variance in the data. Let's add more hidden units and see if we can recover the first n PCs that would be found with PCA. \n",
    "Once again, explore how timecourse depend on   \n",
    "\n",
    "    a) (sign of weights at init)\n",
    "    b) init weight scale\n",
    "    c) context scale ?\n",
    "    \n",
    "The GHA extends Oja's rule to multiple units by adding Gram-Schmidt Orthogonalisation, so that each hidden unit learns a separate principal component: \n",
    "$$ \\Delta \\textbf{W} = \\eta \\left[ \\textbf{y} \\circ \\textbf{x} - tril(\\textbf{y} \\circ \\textbf{y})\\textbf{W}^T\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now with GHA\n",
    "from copy import deepcopy\n",
    "eta = 5e-2\n",
    "sigma = 1e-2\n",
    "n_hidden = 200\n",
    "n_episodes = 10\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=3,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(27,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = W.T@x    \n",
    "    dW = eta*(np.outer(y,x)-np.tril(np.outer(y,y))@W.T).T   \n",
    "    W += dW\n",
    "    delta_ws.append(dW)\n",
    "    ws.append(deepcopy(W))\n",
    "plot_ghasla_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspace Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with SLA rule\n",
    "eta = 5e-2\n",
    "sigma = 1e-5\n",
    "n_hidden = 50\n",
    "n_episodes = 4\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=2,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(27,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = W.T@x\n",
    "    dW = eta*(np.outer(y,x)-np.outer(y,W@y)).T   \n",
    "    W += dW\n",
    "    delta_ws.append(dW)\n",
    "    ws.append(deepcopy(W))\n",
    "plot_ghasla_results()\n",
    "\n",
    "W = ws[0]\n",
    "ya = np.maximum((data['x_test_a']@W),0)\n",
    "yb = np.maximum((data['x_test_b']@W),0)\n",
    "\n",
    "print(f'Before training: \\n task A only: {np.mean((ya.mean(0)>0) & (yb.mean(0)==0))},task B only: {np.mean((ya.mean(0)==0) & (yb.mean(0)>0))}')\n",
    "\n",
    "W = ws[-1]\n",
    "ya = np.maximum((data['x_test_a']@W),0)\n",
    "yb = np.maximum((data['x_test_b']@W),0)\n",
    "print(f'After training: \\n task A only: {np.mean((ya.mean(0)>0) & (yb.mean(0)==0))},task B only: {np.mean((ya.mean(0)==0) & (yb.mean(0)>0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "yh = np.maximum(data['x_all']@ws[-1],0)\n",
    "\n",
    "f,axs = plt.subplots(5,10,figsize=(5,3),dpi=300)\n",
    "axs = axs.flatten()\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(yh[:25,i].reshape((5,5)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.suptitle('task a response')\n",
    "plt.tight_layout()\n",
    "\n",
    "f,axs = plt.subplots(5,10,figsize=(5,3),dpi=300)\n",
    "axs = axs.flatten()    \n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(yh[25:,i].reshape((5,5)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.suptitle('task b response')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: only centering sufficient when weight scale large, as it prepartitions the hidden layer (makes sense)\n",
    " but I can tweak hp so that SLA increases partitioning\n",
    " note how this happens already during task 1 (rationale: inhibition of neurons that are not related to the task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLA with a single context unit (for sign flip)\n",
    "The issue with two context units and centered data is leakage of information about the second task: the ctx inputs are 1,-1 and -1,1 (for the first and second task respectively), instead of 1,0 and 0,1. Semantically, this means \"perform the first and not the second task\", which only makes sense if the network knows a priori how many tasks to perform. True continual learning lacks this information.\n",
    "For now, try to fix this by having only one context unit, which performs a sign flip between task A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with SLA rule\n",
    "eta = 2e-2\n",
    "sigma = 1e-5\n",
    "n_hidden = 200\n",
    "n_episodes = 10\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=3,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "data['x_train'] = data['x_train'][:,:-1]\n",
    "print(data['x_train'].shape)\n",
    "assert data['x_train'].shape[1]==26\n",
    "data['x_test_a'] = data['x_test_a'][:,:-1]\n",
    "data['x_test_b'] = data['x_test_b'][:,:-1]\n",
    "W = np.random.randn(26,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train']\n",
    "for x in X:\n",
    "    y = W.T@x\n",
    "    dW = eta*(np.outer(y,x)-np.outer(y,W@y)).T   \n",
    "    W += dW\n",
    "    delta_ws.append(dW)\n",
    "    ws.append(deepcopy(W))\n",
    "plot_ghasla_results()\n",
    "\n",
    "W = ws[0]\n",
    "ya = np.maximum((data['x_test_a']@W),0)\n",
    "yb = np.maximum((data['x_test_b']@W),0)\n",
    "print(f'Before training: \\n task A only: {np.mean((ya.mean(0)>0) & (yb.mean(0)==0))},task B only: {np.mean((ya.mean(0)==0) & (yb.mean(0)>0))}')\n",
    "W = ws[-1]\n",
    "ya = np.maximum((data['x_test_a']@W),0)\n",
    "yb = np.maximum((data['x_test_b']@W),0)\n",
    "print(f'After training: \\n task A only: {np.mean((ya.mean(0)>0) & (yb.mean(0)==0))},task B only: {np.mean((ya.mean(0)==0) & (yb.mean(0)>0))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now Oja only on ctx weights\n",
    "%matplotlib inline\n",
    "eta = 5e-1\n",
    "sigma = 1e-3\n",
    "n_hidden = 100\n",
    "n_episodes = 4\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=1,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(2,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train'][:,-2:]\n",
    "for x in X:\n",
    "    wlist = []\n",
    "    dwlist = []\n",
    "    for w in W.T:\n",
    "        y = w@x \n",
    "     \n",
    "\n",
    "        dw = eta*y*(x-y*w)    \n",
    "        w += dw\n",
    "        dwlist.append(dw)\n",
    "        wlist.append(deepcopy(w))\n",
    "    dwlist = np.array(dwlist)\n",
    "    \n",
    "    wlist = np.array(wlist).T\n",
    "    \n",
    "     \n",
    "    \n",
    "    delta_ws.append(dwlist)\n",
    "    ws.append(wlist)\n",
    "# plot_ghasla_results()\n",
    "\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster Oja\n",
    "%matplotlib inline\n",
    "eta = 5e-1\n",
    "sigma = 1e-3\n",
    "n_hidden = 100\n",
    "n_episodes = 4\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=1,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(2,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train'][:,-2:]\n",
    "for x in X:\n",
    "    wlist = []\n",
    "    dwlist = []\n",
    "    x_vec = np.tile(x[:,np.newaxis],100)    \n",
    "    assert x_vec.T.shape == (100,2 )\n",
    "    \n",
    "    y = W.T@x\n",
    "    \n",
    "    dW = eta*y*(x_vec-y*W)    \n",
    "    W += dW\n",
    "    delta_ws.append(dW)\n",
    "    ws.append(W)\n",
    "\n",
    "plot_ghasla_results()\n",
    "\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster , no 2nd task knowledge during 1st\n",
    "%matplotlib inline\n",
    "eta = 5e-1\n",
    "sigma = 1e-3\n",
    "n_hidden = 100\n",
    "n_episodes = 4\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=1,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(2,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train'][:,-2:]\n",
    "X[X[:,0]>0,1] = 0\n",
    "\n",
    "\n",
    "for x in X:\n",
    "    wlist = []\n",
    "    dwlist = []\n",
    "    x_vec = np.tile(x[:,np.newaxis],100)    \n",
    "    assert x_vec.T.shape == (100,2 )\n",
    "    \n",
    "    y = W.T@x\n",
    "    \n",
    "    dW = eta*y*(x_vec-y*W)    \n",
    "    W += dW\n",
    "    delta_ws.append(dW)\n",
    "    ws.append(W)\n",
    "\n",
    "plot_ghasla_results()\n",
    "\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now Oja on all weights\n",
    "eta = 5e-2\n",
    "sigma = 1e-3\n",
    "n_hidden = 200\n",
    "n_episodes = 4\n",
    "n_trials = n_episodes*50\n",
    "data = make_blobs_dataset(n_episodes=n_episodes,ctx_scaling=5,training_schedule='blocked',ctx_avg=False,centering=True)\n",
    "W = np.random.randn(27,n_hidden)*sigma\n",
    "delta_ws = []\n",
    "ws = []\n",
    "delta_ws.append(0)\n",
    "ws.append(deepcopy(W))\n",
    "X = data['x_train'][:,:]\n",
    "for x in X:\n",
    "    wlist = []\n",
    "    dwlist = []\n",
    "    for w in W.T:\n",
    "        y = w.T@x    \n",
    "        dw = eta*y*(x-y*w.T)    \n",
    "        w += dw\n",
    "        dwlist.append(dw)\n",
    "        wlist.append(deepcopy(w))\n",
    "    dwlist = np.array(dwlist)\n",
    "    \n",
    "    wlist = np.array(wlist).T\n",
    "    \n",
    "    \n",
    "    \n",
    "    delta_ws.append(dwlist)\n",
    "    ws.append(wlist)\n",
    "plot_ghasla_results()\n",
    "plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# set stimulus weights to init, to assess whether oja induces any stim selectivity (shouldn't)\n",
    "ws[-1][:25,:] = ws[0][:25,:]\n",
    "yh = np.maximum(data['x_all']@ws[-1],0)\n",
    "\n",
    "f,axs = plt.subplots(5,10,figsize=(4,2.5),dpi=300)\n",
    "axs = axs.flatten()\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(yh[:25,i].reshape((5,5)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.suptitle('task a response')\n",
    "f.set_facecolor('w')\n",
    "plt.tight_layout()\n",
    "\n",
    "f,axs = plt.subplots(5,10,figsize=(4,2.5),dpi=300)\n",
    "axs = axs.flatten()    \n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(yh[25:,i].reshape((5,5)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.suptitle('task b response')\n",
    "f.set_facecolor('w')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
