{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad for paper revisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import os, sys\n",
    "root_path = os.path.realpath('../')\n",
    "sys.path.append(root_path)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from utils.data import make_blobs_dataset, make_trees_dataset\n",
    "from utils.nnet import get_device\n",
    "\n",
    "from hebbcl.logger import LoggerFactory\n",
    "from hebbcl.model import Nnet, ScaledNet2Hidden\n",
    "from hebbcl.trainer import Optimiser, train_on_blobs, train_on_trees\n",
    "from hebbcl.parameters import parser\n",
    "from hebbcl.tuner import HPOTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation\n",
    "hpo on network trained with fewer episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO: blocked trials with oja_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO on blocked trials with oja_ctx\n",
    "args = parser.parse_args(args=[])\n",
    "args.n_episodes = 8\n",
    "args.hpo_fixedseed = True\n",
    "args.hpo_scheduler = \"bohb\"\n",
    "args.hpo_searcher = \"bohb\"\n",
    "# dict(sorted(vars(args).items(),key=lambda k: k[0]))\n",
    "args.ctx_avg = False\n",
    "# init tuner\n",
    "tuner = HPOTuner(args, time_budget=60*15, metric=\"loss\")\n",
    "\n",
    "tuner.tune(n_samples=500)\n",
    "\n",
    "df = tuner.results\n",
    "df = df[[\"mean_loss\", \"mean_acc\", \"config.lrate_sgd\",\"config.lrate_hebb\", \"config.ctx_scaling\",\"config.seed\",\"done\"]]\n",
    "df = df[df[\"done\"]==True]\n",
    "df = df.drop(columns=[\"done\"])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(\"mean_loss\",ascending=True)\n",
    "\n",
    "df.reset_index()\n",
    "print(df.head(15))\n",
    "\n",
    "print(tuner.best_cfg)\n",
    "\n",
    "with open(\"../results/raytune_oja_ctx_blocked_8episodes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/raytune_oja_ctx_blocked_8episodes.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify results \n",
    "with open(\"../results/raytune_oja_ctx_blocked_8episodes.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "args.n_episodes = 8\n",
    "args.lrate_hebb = df.iloc[0][\"config.lrate_hebb\"]\n",
    "args.lrate_sgd = df.iloc[0][\"config.lrate_sgd\"]\n",
    "args.ctx_scaling = df.iloc[0][\"config.ctx_scaling\"]\n",
    "args.ctx_avg = False\n",
    "np.random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "torch.manual_seed(int(df.iloc[0][\"config.seed\"]))\n",
    "\n",
    "\n",
    "# create dataset \n",
    "dataset = make_blobs_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = LoggerFactory.create(args, save_dir)\n",
    "model = Nnet(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_on_blobs(args, model, optimiser, dataset, logger)\n",
    "\n",
    "print(f\"config: lrate_sgd: {args.lrate_sgd:.4f}, lrate_hebb: {args.lrate_hebb:.4f}, context offset: {args.ctx_scaling}\")\n",
    "print(f\"terminal accuracy: {logger.results['acc_total'][-1]:.2f}, loss: {logger.results['losses_total'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO: Interleaved trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO on blocked trials with oja_ctx\n",
    "args = parser.parse_args(args=[])\n",
    "args.n_episodes = 8\n",
    "args.hpo_fixedseed = True\n",
    "args.hpo_scheduler = \"bohb\"\n",
    "args.hpo_searcher = \"bohb\"\n",
    "args.training_schedule = \"interleaved\"\n",
    "# dict(sorted(vars(args).items(),key=lambda k: k[0]))\n",
    "args.ctx_avg = False\n",
    "# init tuner\n",
    "tuner = HPOTuner(args, time_budget=60*15, metric=\"loss\")\n",
    "\n",
    "tuner.tune(n_samples=500)\n",
    "\n",
    "df = tuner.results\n",
    "df = df[[\"mean_loss\", \"mean_acc\", \"config.lrate_sgd\",\"config.lrate_hebb\", \"config.ctx_scaling\",\"config.seed\",\"done\"]]\n",
    "df = df[df[\"done\"]==True]\n",
    "df = df.drop(columns=[\"done\"])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(\"mean_loss\",ascending=True)\n",
    "\n",
    "df.reset_index()\n",
    "print(df.head(15))\n",
    "\n",
    "print(tuner.best_cfg)\n",
    "\n",
    "with open(\"../results/raytune_oja_ctx_interleaved_8episodes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify results \n",
    "\n",
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "args.n_episodes = 8\n",
    "args.lrate_hebb = df.iloc[0][\"config.lrate_hebb\"]\n",
    "args.lrate_sgd = df.iloc[0][\"config.lrate_sgd\"]\n",
    "args.ctx_scaling = df.iloc[0][\"config.ctx_scaling\"]\n",
    "args.ctx_avg = False\n",
    "args.training_schedule = \"interleaved\"\n",
    "np.random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "torch.manual_seed(int(df.iloc[0][\"config.seed\"]))\n",
    "\n",
    "\n",
    "\n",
    "# create dataset \n",
    "dataset = make_blobs_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = LoggerFactory.create(args, save_dir)\n",
    "model = Nnet(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_on_blobs(args, model, optimiser, dataset, logger)\n",
    "\n",
    "print(f\"config: lrate_sgd: {args.lrate_sgd:.4f}, lrate_hebb: {args.lrate_hebb:.4f}, context offset: {args.ctx_scaling}\")\n",
    "print(f\"terminal accuracy: {logger.results['acc_total'][-1]:.2f}, loss: {logger.results['losses_total'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO: all_oja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>done</th>\n",
       "      <th>config.lrate_sgd</th>\n",
       "      <th>config.lrate_hebb</th>\n",
       "      <th>config.ctx_scaling</th>\n",
       "      <th>config.seed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dd895_02286</th>\n",
       "      <td>-7244.266113</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>3</td>\n",
       "      <td>8102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_01058</th>\n",
       "      <td>-7132.384277</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>3</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00485</th>\n",
       "      <td>-7290.099609</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>5</td>\n",
       "      <td>4620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_02205</th>\n",
       "      <td>-7279.646484</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>3</td>\n",
       "      <td>2612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_02063</th>\n",
       "      <td>-7140.266113</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>5</td>\n",
       "      <td>9754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_02154</th>\n",
       "      <td>-7094.134766</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>3</td>\n",
       "      <td>5437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00377</th>\n",
       "      <td>-7031.128906</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>5</td>\n",
       "      <td>9768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00689</th>\n",
       "      <td>-6842.807617</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>3</td>\n",
       "      <td>9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00642</th>\n",
       "      <td>-6931.358887</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>5</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_01279</th>\n",
       "      <td>-6663.882324</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>5</td>\n",
       "      <td>3905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00639</th>\n",
       "      <td>-6766.567871</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>4</td>\n",
       "      <td>6945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_01891</th>\n",
       "      <td>-6334.618652</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>3</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_01826</th>\n",
       "      <td>-6559.426270</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>4</td>\n",
       "      <td>7617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_00174</th>\n",
       "      <td>-6404.856445</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>4</td>\n",
       "      <td>4568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd895_02224</th>\n",
       "      <td>-6378.503906</td>\n",
       "      <td>0.9225</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>4</td>\n",
       "      <td>7392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean_loss  mean_acc  done  config.lrate_sgd  config.lrate_hebb  \\\n",
       "trial_id                                                                        \n",
       "dd895_02286 -7244.266113    0.9825  True          0.003509           0.000137   \n",
       "dd895_01058 -7132.384277    0.9825  True          0.000845           0.000225   \n",
       "dd895_00485 -7290.099609    0.9800  True          0.003067           0.000120   \n",
       "dd895_02205 -7279.646484    0.9775  True          0.001809           0.000104   \n",
       "dd895_02063 -7140.266113    0.9725  True          0.000774           0.000134   \n",
       "dd895_02154 -7094.134766    0.9700  True          0.000993           0.000187   \n",
       "dd895_00377 -7031.128906    0.9625  True          0.003907           0.000170   \n",
       "dd895_00689 -6842.807617    0.9550  True          0.001442           0.000181   \n",
       "dd895_00642 -6931.358887    0.9550  True          0.000973           0.000113   \n",
       "dd895_01279 -6663.882324    0.9500  True          0.000644           0.000148   \n",
       "dd895_00639 -6766.567871    0.9475  True          0.000864           0.000233   \n",
       "dd895_01891 -6334.618652    0.9450  True          0.000681           0.000324   \n",
       "dd895_01826 -6559.426270    0.9275  True          0.002664           0.000148   \n",
       "dd895_00174 -6404.856445    0.9275  True          0.000769           0.000467   \n",
       "dd895_02224 -6378.503906    0.9225  True          0.001238           0.000373   \n",
       "\n",
       "             config.ctx_scaling  config.seed  \n",
       "trial_id                                      \n",
       "dd895_02286                   3         8102  \n",
       "dd895_01058                   3          802  \n",
       "dd895_00485                   5         4620  \n",
       "dd895_02205                   3         2612  \n",
       "dd895_02063                   5         9754  \n",
       "dd895_02154                   3         5437  \n",
       "dd895_00377                   5         9768  \n",
       "dd895_00689                   3         9170  \n",
       "dd895_00642                   5          894  \n",
       "dd895_01279                   5         3905  \n",
       "dd895_00639                   4         6945  \n",
       "dd895_01891                   3          429  \n",
       "dd895_01826                   4         7617  \n",
       "dd895_00174                   4         4568  \n",
       "dd895_02224                   4         7392  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../results/raytune_trees_asha_interleaved_ojaall_1ctx.pkl\",\"rb\") as f:\n",
    "    df = pickle.load(f)[\"df\"]\n",
    "df = df.sort_values(\"mean_acc\",ascending=False).head(15)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "step 0, loss: task a -0.4216, task b 0.3495 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 5 n_b: 4\n",
      "... 2nd hidden: n_a: 7 n_b: 7\n",
      "step 50, loss: task a 0.1933, task b -1.9896 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 8 n_b: 6\n",
      "... 2nd hidden: n_a: 1 n_b: 9\n",
      "step 100, loss: task a -1.1461, task b -12.2039 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 6 n_b: 7\n",
      "... 2nd hidden: n_a: 2 n_b: 3\n",
      "step 150, loss: task a -7.9329, task b -36.3379 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 3 n_b: 2\n",
      "... 2nd hidden: n_a: 1 n_b: 2\n",
      "step 200, loss: task a -8.0407, task b -44.7073 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 3 n_b: 2\n",
      "... 2nd hidden: n_a: 1 n_b: 1\n",
      "step 250, loss: task a -34.5834, task b -113.7178 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 1 n_b: 3\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 300, loss: task a -122.8599, task b -120.6186 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 1 n_b: 3\n",
      "... 2nd hidden: n_a: 1 n_b: 2\n",
      "step 350, loss: task a -336.9130, task b -496.4633 | acc: task a 0.5000, task b 0.5000\n",
      "...1st hidden: n_a: 0 n_b: 0\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 400, loss: task a -900.6320, task b -875.3535 | acc: task a 0.5200, task b 0.6200\n",
      "...1st hidden: n_a: 0 n_b: 1\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 450, loss: task a -1016.9380, task b -1765.0420 | acc: task a 0.7200, task b 0.8900\n",
      "...1st hidden: n_a: 0 n_b: 0\n",
      "... 2nd hidden: n_a: 0 n_b: 1\n",
      "step 500, loss: task a -1617.9528, task b -3349.0581 | acc: task a 0.7400, task b 0.9950\n",
      "...1st hidden: n_a: 0 n_b: 0\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 550, loss: task a -1884.7600, task b -3474.3730 | acc: task a 0.7500, task b 0.9900\n",
      "...1st hidden: n_a: 1 n_b: 0\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 600, loss: task a -2286.0640, task b -3338.4429 | acc: task a 0.7800, task b 0.9400\n",
      "...1st hidden: n_a: 0 n_b: 0\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 650, loss: task a -2131.5471, task b -3673.5640 | acc: task a 0.7700, task b 0.9950\n",
      "...1st hidden: n_a: 1 n_b: 1\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 700, loss: task a -2115.0369, task b -3692.7998 | acc: task a 0.7700, task b 0.9950\n",
      "...1st hidden: n_a: 1 n_b: 1\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 750, loss: task a -2958.2131, task b -3631.4958 | acc: task a 0.8950, task b 0.9850\n",
      "...1st hidden: n_a: 0 n_b: 0\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 800, loss: task a -2987.9241, task b -3687.3506 | acc: task a 0.9000, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 2\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 850, loss: task a -3370.4456, task b -3678.3328 | acc: task a 0.9450, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 1\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 900, loss: task a -3411.5120, task b -3615.6899 | acc: task a 0.9600, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 2\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 950, loss: task a -3413.1382, task b -3657.5193 | acc: task a 0.9350, task b 0.9950\n",
      "...1st hidden: n_a: 1 n_b: 2\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1000, loss: task a -3359.1353, task b -3636.1853 | acc: task a 0.9350, task b 0.9900\n",
      "...1st hidden: n_a: 4 n_b: 3\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1050, loss: task a -2819.8977, task b -3709.2959 | acc: task a 0.8650, task b 0.9950\n",
      "...1st hidden: n_a: 8 n_b: 9\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1100, loss: task a -3366.3850, task b -3727.5381 | acc: task a 0.9350, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 5\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1150, loss: task a -3323.1448, task b -3726.7266 | acc: task a 0.9350, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 5\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1200, loss: task a -3410.3274, task b -3729.7483 | acc: task a 0.9450, task b 0.9950\n",
      "...1st hidden: n_a: 5 n_b: 10\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1250, loss: task a -3549.9014, task b -3713.3120 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 0 n_b: 10\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1300, loss: task a -3611.2942, task b -3725.3262 | acc: task a 0.9800, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1350, loss: task a -3628.1914, task b -3725.1868 | acc: task a 0.9850, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1400, loss: task a -3376.2310, task b -3722.8672 | acc: task a 0.9400, task b 0.9950\n",
      "...1st hidden: n_a: 11 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1450, loss: task a -3633.4451, task b -3726.2173 | acc: task a 0.9900, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1500, loss: task a -3611.6602, task b -3726.8823 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1550, loss: task a -3588.6663, task b -3726.9380 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1600, loss: task a -3574.1272, task b -3726.5833 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 4 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1650, loss: task a -3595.7119, task b -3725.9087 | acc: task a 0.9800, task b 0.9950\n",
      "...1st hidden: n_a: 6 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1700, loss: task a -3210.9348, task b -3722.6106 | acc: task a 0.9150, task b 0.9950\n",
      "...1st hidden: n_a: 12 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1750, loss: task a -3586.4912, task b -3717.6921 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 9 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1800, loss: task a -3568.2585, task b -3726.7922 | acc: task a 0.9700, task b 0.9950\n",
      "...1st hidden: n_a: 8 n_b: 11\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 1850, loss: task a -3595.6082, task b -3743.1677 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 12\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 1900, loss: task a -3616.9043, task b -3742.6858 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 11 n_b: 12\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 1950, loss: task a -3341.2710, task b -3741.2344 | acc: task a 0.9200, task b 1.0000\n",
      "...1st hidden: n_a: 2 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2000, loss: task a -2905.7080, task b -3734.7258 | acc: task a 0.8650, task b 0.9950\n",
      "...1st hidden: n_a: 1 n_b: 12\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2050, loss: task a -3611.8027, task b -3722.0315 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 2 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2100, loss: task a -3599.3682, task b -3727.4087 | acc: task a 0.9750, task b 1.0000\n",
      "...1st hidden: n_a: 2 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2150, loss: task a -3612.4192, task b -3733.0354 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 2 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2200, loss: task a -3533.0396, task b -3730.6729 | acc: task a 0.9700, task b 1.0000\n",
      "...1st hidden: n_a: 1 n_b: 7\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2250, loss: task a -3581.5623, task b -3729.5972 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 3 n_b: 8\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2300, loss: task a -3576.9192, task b -3728.9338 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 5 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2350, loss: task a -3581.5732, task b -3728.1277 | acc: task a 0.9750, task b 0.9950\n",
      "...1st hidden: n_a: 2 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2400, loss: task a -3483.5481, task b -3727.3210 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 1 n_b: 7\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2450, loss: task a -3369.1804, task b -3744.2493 | acc: task a 0.9300, task b 1.0000\n",
      "...1st hidden: n_a: 2 n_b: 16\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2500, loss: task a -3613.0552, task b -3738.9641 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 5 n_b: 15\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2550, loss: task a -3595.5950, task b -3703.8000 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 6 n_b: 18\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2600, loss: task a -3606.3281, task b -3713.4055 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 6 n_b: 18\n",
      "... 2nd hidden: n_a: 0 n_b: 0\n",
      "step 2650, loss: task a -3117.3804, task b -3697.6870 | acc: task a 0.8800, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2700, loss: task a -1748.2727, task b -3689.6091 | acc: task a 0.6900, task b 0.9950\n",
      "...1st hidden: n_a: 10 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2750, loss: task a -3031.6479, task b -3169.4700 | acc: task a 0.8650, task b 0.9400\n",
      "...1st hidden: n_a: 14 n_b: 20\n",
      "... 2nd hidden: n_a: 3 n_b: 0\n",
      "step 2800, loss: task a -3071.0867, task b -3744.4407 | acc: task a 0.8700, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 2 n_b: 0\n",
      "step 2850, loss: task a -3249.7227, task b -3745.1245 | acc: task a 0.9200, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 2 n_b: 0\n",
      "step 2900, loss: task a -3288.2659, task b -3744.1299 | acc: task a 0.9250, task b 1.0000\n",
      "...1st hidden: n_a: 13 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 2950, loss: task a -3403.1587, task b -3741.9680 | acc: task a 0.9400, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3000, loss: task a -3547.7190, task b -3740.0483 | acc: task a 0.9600, task b 1.0000\n",
      "...1st hidden: n_a: 4 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3050, loss: task a -3471.2285, task b -3734.7139 | acc: task a 0.9450, task b 0.9950\n",
      "...1st hidden: n_a: 6 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3100, loss: task a -3559.1733, task b -3732.9312 | acc: task a 0.9600, task b 0.9950\n",
      "...1st hidden: n_a: 6 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3150, loss: task a -3528.0522, task b -3731.4885 | acc: task a 0.9650, task b 0.9950\n",
      "...1st hidden: n_a: 8 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3200, loss: task a -3511.8560, task b -3729.8777 | acc: task a 0.9600, task b 0.9950\n",
      "...1st hidden: n_a: 9 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3250, loss: task a -3493.8008, task b -3728.6938 | acc: task a 0.9600, task b 0.9950\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3300, loss: task a -3476.9324, task b -3727.7732 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3350, loss: task a -3462.5146, task b -3727.0203 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3400, loss: task a -3451.0781, task b -3726.4409 | acc: task a 0.9500, task b 0.9950\n",
      "...1st hidden: n_a: 12 n_b: 18\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3450, loss: task a -3442.7734, task b -3725.9878 | acc: task a 0.9500, task b 0.9950\n",
      "...1st hidden: n_a: 13 n_b: 17\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3500, loss: task a -3443.5449, task b -3725.5166 | acc: task a 0.9500, task b 0.9950\n",
      "...1st hidden: n_a: 14 n_b: 16\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3550, loss: task a -3491.4551, task b -3725.0933 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 13 n_b: 16\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3600, loss: task a -3516.3850, task b -3724.3330 | acc: task a 0.9650, task b 0.9950\n",
      "...1st hidden: n_a: 14 n_b: 16\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3650, loss: task a -3486.4023, task b -3722.9800 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 14 n_b: 16\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3700, loss: task a -3475.4822, task b -3722.3516 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 14 n_b: 16\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3750, loss: task a -3479.4312, task b -3725.3313 | acc: task a 0.9550, task b 0.9950\n",
      "...1st hidden: n_a: 14 n_b: 17\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3800, loss: task a -3351.4292, task b -3749.9526 | acc: task a 0.9400, task b 1.0000\n",
      "...1st hidden: n_a: 14 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3850, loss: task a -3359.9326, task b -3749.9460 | acc: task a 0.9400, task b 1.0000\n",
      "...1st hidden: n_a: 14 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3900, loss: task a -3570.1714, task b -3749.9026 | acc: task a 0.9700, task b 1.0000\n",
      "...1st hidden: n_a: 11 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 3950, loss: task a -3557.3467, task b -3749.8674 | acc: task a 0.9650, task b 1.0000\n",
      "...1st hidden: n_a: 11 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4000, loss: task a -3642.2776, task b -3749.7241 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4050, loss: task a -3620.9666, task b -3749.5823 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 1 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4100, loss: task a -3641.5422, task b -3749.4258 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 3 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4150, loss: task a -3655.0579, task b -3748.5530 | acc: task a 0.9900, task b 1.0000\n",
      "...1st hidden: n_a: 10 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4200, loss: task a -3673.7993, task b -3747.8931 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 10 n_b: 13\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4250, loss: task a -3645.4026, task b -3746.6404 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 13\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4300, loss: task a -3511.1829, task b -3744.9890 | acc: task a 0.9650, task b 1.0000\n",
      "...1st hidden: n_a: 12 n_b: 12\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4350, loss: task a -3680.7104, task b -3742.7141 | acc: task a 0.9900, task b 1.0000\n",
      "...1st hidden: n_a: 8 n_b: 12\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4400, loss: task a -3675.2351, task b -3740.8025 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 7 n_b: 12\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4450, loss: task a -3565.9561, task b -3736.6541 | acc: task a 0.9700, task b 0.9950\n",
      "...1st hidden: n_a: 0 n_b: 11\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4500, loss: task a -3623.1021, task b -3747.6294 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4550, loss: task a -3631.0823, task b -3748.6487 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4600, loss: task a -3628.9795, task b -3749.7534 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4650, loss: task a -3629.9258, task b -3749.7964 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4700, loss: task a -3624.3657, task b -3749.8616 | acc: task a 0.9850, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4750, loss: task a -3619.0486, task b -3749.8958 | acc: task a 0.9800, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4800, loss: task a -3723.2742, task b -3749.9294 | acc: task a 1.0000, task b 1.0000\n",
      "...1st hidden: n_a: 1 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4850, loss: task a -3721.3059, task b -3749.9492 | acc: task a 1.0000, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4900, loss: task a -3696.3552, task b -3749.9573 | acc: task a 0.9900, task b 1.0000\n",
      "...1st hidden: n_a: 0 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "step 4950, loss: task a -3698.1287, task b -3749.9690 | acc: task a 0.9900, task b 1.0000\n",
      "...1st hidden: n_a: 1 n_b: 14\n",
      "... 2nd hidden: n_a: 1 n_b: 0\n",
      "done\n",
      "config: lrate_sgd: 0.0035, lrate_hebb: 0.0001, context offset: 3\n",
      "terminal accuracy: 1.00, loss: -7448.10\n"
     ]
    }
   ],
   "source": [
    "# verify results \n",
    "\n",
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "args.n_episodes = 200\n",
    "args.n_layers = 2\n",
    "args.n_features = 974\n",
    "args.lrate_hebb = df.iloc[0][\"config.lrate_hebb\"]\n",
    "args.lrate_sgd = df.iloc[0][\"config.lrate_sgd\"]\n",
    "args.ctx_scaling = df.iloc[0][\"config.ctx_scaling\"]\n",
    "args.ctx_avg = False\n",
    "args.training_schedule = \"interleaved\"\n",
    "np.random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "random.seed(int(df.iloc[0][\"config.seed\"]))\n",
    "torch.manual_seed(int(df.iloc[0][\"config.seed\"]))\n",
    "\n",
    "\n",
    "\n",
    "# create dataset \n",
    "dataset = make_trees_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = LoggerFactory.create(args, save_dir)\n",
    "model = ScaledNet2Hidden(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_on_trees(args, model, optimiser, dataset, logger)\n",
    "\n",
    "print(f\"config: lrate_sgd: {args.lrate_sgd:.4f}, lrate_hebb: {args.lrate_hebb:.4f}, context offset: {args.ctx_scaling}\")\n",
    "print(f\"terminal accuracy: {logger.results['acc_total'][-1]:.2f}, loss: {logger.results['losses_total'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  1.0min\n"
     ]
    }
   ],
   "source": [
    "# test tune validator\n",
    "\n",
    "from hebbcl.tuner import validate_tuner_results\n",
    "\n",
    "validate_tuner_results(filename=\"trees_asha_interleaved_vanilla_1ctx\", \n",
    "filepath=\"../results/\",datapath=\"../datasets/\",datasuffix=\"_ds18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import resize_images\n",
    "for c in [\"training\", \"test\"]:\n",
    "    for g in [\"north\", \"south\"]:\n",
    "        resize_images(c+\"_data_\"+g +\"_withgarden\", size=[24,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.n_features = 24*24*3+2\n",
    "args.n_layers = 2\n",
    "data = make_trees_dataset(args,filesuffix=\"_withgarden_ds24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25f98e9e520>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[\"x_test_a\"][0,:-2].reshape((24,24,3)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a3ba60a28f1899318f4810ee01fef19e535f7a46e788980dcac9bebef4b464e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
