{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad for paper revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import os, sys\n",
    "root_path = os.path.realpath('../')\n",
    "sys.path.append(root_path)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from utils.data import make_dataset\n",
    "from utils.nnet import get_device\n",
    "\n",
    "from hebbcl.logger import MetricLogger\n",
    "from hebbcl.model import Nnet\n",
    "from hebbcl.trainer import Optimiser, train_model\n",
    "from hebbcl.parameters import parser\n",
    "from hebbcl.tuner import HPOTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation\n",
    "hpo on network trained with fewer episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO: blocked trials with oja_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-23 14:16:46 (running for 00:15:03.36)<br>Memory usage on this node: 8.5/15.5 GiB<br>Using HyperBand: num_stopped=328 total_brackets=4\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=11, Milestone (r)=27, completed=32.3%): {TERMINATED: 284} \n",
       "  Bracket(Max Size (n)=13, Milestone (r)=45, completed=21.3%): {TERMINATED: 114} \n",
       "  Bracket(Max Size (n)=48, Milestone (r)=16, completed=11.8%): {TERMINATED: 50} <br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/5.21 GiB heap, 0.0/2.6 GiB objects<br>Current best trial: 16dfc485 with mean_loss=-28.530746459960938 and parameters={'lrate_sgd': 0.09207067771676251, 'lrate_hebb': 0.0039883754510576805, 'ctx_scaling': 3, 'seed': 9531}<br>Result logdir: C:\\Users\\Timo\\ray_results\\lambda_2022-06-23_14-01-43<br>Number of trials: 448/500 (448 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 14:16:47,068\tINFO tune.py:636 -- Total run time: 903.65 seconds (900.65 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean_loss  mean_acc  config.lrate_sgd  config.lrate_hebb  \\\n",
      "trial_id                                                             \n",
      "16dfc485 -28.530746       1.0          0.092071           0.003988   \n",
      "087fa24c -28.298903       1.0          0.090435           0.020123   \n",
      "3cd50ef9 -28.293268       1.0          0.091203           0.003310   \n",
      "03647346 -28.250048       1.0          0.088752           0.003588   \n",
      "ad5ea28f -28.114159       1.0          0.090982           0.002755   \n",
      "8b668d93 -28.114054       1.0          0.091786           0.004020   \n",
      "1a1448e1 -28.077969       1.0          0.091507           0.002960   \n",
      "39b8189a -28.058559       1.0          0.091538           0.002974   \n",
      "996504df -27.999535       1.0          0.090843           0.003099   \n",
      "d532aa5d -27.950037       1.0          0.088303           0.006702   \n",
      "381b5ce4 -27.905373       1.0          0.067527           0.094744   \n",
      "cb22fad4 -27.878670       1.0          0.078666           0.013706   \n",
      "aa49ea84 -27.869978       1.0          0.091265           0.003055   \n",
      "39d6fd22 -27.868721       1.0          0.088740           0.003563   \n",
      "b312e1e8 -27.792665       1.0          0.083653           0.003193   \n",
      "\n",
      "          config.ctx_scaling  config.seed  \n",
      "trial_id                                   \n",
      "16dfc485                   3         9531  \n",
      "087fa24c                   2         5615  \n",
      "3cd50ef9                   4         9828  \n",
      "03647346                   3         6378  \n",
      "ad5ea28f                   4         9368  \n",
      "8b668d93                   3         9370  \n",
      "1a1448e1                   3         8933  \n",
      "39b8189a                   3         8168  \n",
      "996504df                   3         8650  \n",
      "d532aa5d                   3         4710  \n",
      "381b5ce4                   1         7260  \n",
      "cb22fad4                   2         9797  \n",
      "aa49ea84                   3         8487  \n",
      "39d6fd22                   3         6696  \n",
      "b312e1e8                   3         3433  \n",
      "{'lrate_sgd': 0.09207067771676251, 'lrate_hebb': 0.0039883754510576805, 'ctx_scaling': 3, 'seed': 9531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Timo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ray\\tune\\analysis\\experiment_analysis.py:280: UserWarning: Dataframes will use '/' instead of '.' to delimit nested result keys in future versions of Ray. For forward compatibility, set the environment variable TUNE_RESULT_DELIM='/'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# HPO on blocked trials with oja_ctx\n",
    "args = parser.parse_args(args=[])\n",
    "args.n_episodes = 8\n",
    "args.hpo_fixedseed = True\n",
    "args.hpo_scheduler = \"bohb\"\n",
    "args.hpo_searcher = \"bohb\"\n",
    "# dict(sorted(vars(args).items(),key=lambda k: k[0]))\n",
    "args.ctx_avg = False\n",
    "# init tuner\n",
    "tuner = HPOTuner(args, time_budget=60*15, metric=\"loss\")\n",
    "\n",
    "tuner.tune(n_samples=500)\n",
    "\n",
    "df = tuner.results\n",
    "df = df[[\"mean_loss\", \"mean_acc\", \"config.lrate_sgd\",\"config.lrate_hebb\", \"config.ctx_scaling\",\"config.seed\",\"done\"]]\n",
    "df = df[df[\"done\"]==True]\n",
    "df = df.drop(columns=[\"done\"])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(\"mean_loss\",ascending=True)\n",
    "\n",
    "df.reset_index()\n",
    "print(df.head(15))\n",
    "\n",
    "print(tuner.best_cfg)\n",
    "\n",
    "with open(\"../results/raytune_oja_ctx_blocked_8episodes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: task a -0.2024, task b -0.1295 | acc: task a 0.5000, task b 0.5000\n",
      "... n_a: 6 n_b: 8\n",
      "step 50, loss: task a -9.7440, task b 0.2153 | acc: task a 1.0000, task b 0.4500\n",
      "... n_a: 17 n_b: 0\n",
      "step 100, loss: task a -13.6514, task b 0.1502 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 23 n_b: 0\n",
      "step 150, loss: task a -14.3797, task b 0.0882 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 24 n_b: 0\n",
      "step 200, loss: task a -14.6141, task b -0.0504 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 22 n_b: 0\n",
      "step 250, loss: task a -14.2739, task b -4.4524 | acc: task a 1.0000, task b 0.8500\n",
      "... n_a: 12 n_b: 0\n",
      "step 300, loss: task a -12.4746, task b -11.7946 | acc: task a 0.9500, task b 1.0000\n",
      "... n_a: 15 n_b: 5\n",
      "step 350, loss: task a -9.3752, task b -14.0712 | acc: task a 0.8500, task b 1.0000\n",
      "... n_a: 15 n_b: 24\n",
      "done\n",
      "config: lrate_sgd: 0.0921, lrate_hebb: 0.0040, context offset: 3\n",
      "terminal accuracy: 0.93, loss: -23.45\n"
     ]
    }
   ],
   "source": [
    "# verify results \n",
    "\n",
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "args.n_episodes = 8\n",
    "args.lrate_hebb = tuner.best_cfg[\"lrate_hebb\"]\n",
    "args.lrate_sgd = tuner.best_cfg[\"lrate_sgd\"]\n",
    "args.ctx_scaling = tuner.best_cfg[\"ctx_scaling\"]\n",
    "args.ctx_avg = False\n",
    "# np.random.seed(tuner.best_cfg[\"seed\"])\n",
    "# random.seed(tuner.best_cfg[\"seed\"])\n",
    "# torch.manual_seed(tuner.best_cfg[\"seed\"])\n",
    "\n",
    "\n",
    "# create dataset \n",
    "dataset = make_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = MetricLogger(save_dir)\n",
    "model = Nnet(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_model(args, model, optimiser, dataset, logger)\n",
    "\n",
    "print(f\"config: lrate_sgd: {args.lrate_sgd:.4f}, lrate_hebb: {args.lrate_hebb:.4f}, context offset: {args.ctx_scaling}\")\n",
    "print(f\"terminal accuracy: {logger.acc_total[-1]:.2f}, loss: {logger.losses_total[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO: Interleaved trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-23 14:32:01 (running for 00:15:02.92)<br>Memory usage on this node: 8.4/15.5 GiB<br>Using HyperBand: num_stopped=328 total_brackets=4\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=11, Milestone (r)=27, completed=32.3%): {TERMINATED: 284} \n",
       "  Bracket(Max Size (n)=13, Milestone (r)=45, completed=21.3%): {TERMINATED: 114} \n",
       "  Bracket(Max Size (n)=48, Milestone (r)=16, completed=14.5%): {TERMINATED: 62} <br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/4.65 GiB heap, 0.0/2.33 GiB objects<br>Current best trial: d46282a5 with mean_loss=-29.282215118408203 and parameters={'lrate_sgd': 0.08710014100174149, 'lrate_hebb': 0.005814333717889643, 'ctx_scaling': 3, 'seed': 1464}<br>Result logdir: C:\\Users\\Timo\\ray_results\\lambda_2022-06-23_14-16-58<br>Number of trials: 460/500 (460 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 14:32:01,270\tINFO tune.py:636 -- Total run time: 903.16 seconds (900.28 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean_loss  mean_acc  config.lrate_sgd  config.lrate_hebb  \\\n",
      "trial_id                                                             \n",
      "d46282a5 -29.282215       1.0          0.087100           0.005814   \n",
      "e95cda9f -29.152634       1.0          0.083062           0.006928   \n",
      "96431736 -28.942841       1.0          0.088884           0.016040   \n",
      "67316b60 -28.915279       1.0          0.089539           0.020199   \n",
      "63c268f6 -28.843061       1.0          0.089837           0.016160   \n",
      "551533f5 -28.838718       1.0          0.094707           0.054363   \n",
      "67180152 -28.759750       1.0          0.087407           0.018329   \n",
      "a3faebae -28.733679       1.0          0.087999           0.013216   \n",
      "97e88225 -28.678537       1.0          0.090390           0.023709   \n",
      "a6beafdd -28.586058       1.0          0.085326           0.004948   \n",
      "9911a805 -28.576925       1.0          0.086460           0.003645   \n",
      "c3f444ad -28.526861       1.0          0.086867           0.003767   \n",
      "67242e0d -28.523685       1.0          0.087067           0.013299   \n",
      "6477968c -28.430763       1.0          0.086250           0.003440   \n",
      "4c7849fb -28.396555       1.0          0.088092           0.012048   \n",
      "\n",
      "          config.ctx_scaling  config.seed  \n",
      "trial_id                                   \n",
      "d46282a5                 3.0       1464.0  \n",
      "e95cda9f                 3.0       7972.0  \n",
      "96431736                 2.0       7847.0  \n",
      "67316b60                 2.0       6015.0  \n",
      "63c268f6                 2.0       6316.0  \n",
      "551533f5                 1.0        494.0  \n",
      "67180152                 2.0       4340.0  \n",
      "a3faebae                 2.0       5053.0  \n",
      "97e88225                 2.0       8046.0  \n",
      "a6beafdd                 3.0        163.0  \n",
      "9911a805                 3.0       3228.0  \n",
      "c3f444ad                 3.0       3696.0  \n",
      "67242e0d                 2.0       7967.0  \n",
      "6477968c                 3.0       2024.0  \n",
      "4c7849fb                 2.0       6591.0  \n",
      "{'lrate_sgd': 0.08710014100174149, 'lrate_hebb': 0.005814333717889643, 'ctx_scaling': 3, 'seed': 1464}\n"
     ]
    }
   ],
   "source": [
    "# HPO on blocked trials with oja_ctx\n",
    "args = parser.parse_args(args=[])\n",
    "args.n_episodes = 8\n",
    "args.hpo_fixedseed = True\n",
    "args.hpo_scheduler = \"bohb\"\n",
    "args.hpo_searcher = \"bohb\"\n",
    "args.training_schedule = \"interleaved\"\n",
    "# dict(sorted(vars(args).items(),key=lambda k: k[0]))\n",
    "args.ctx_avg = False\n",
    "# init tuner\n",
    "tuner = HPOTuner(args, time_budget=60*15, metric=\"loss\")\n",
    "\n",
    "tuner.tune(n_samples=500)\n",
    "\n",
    "df = tuner.results\n",
    "df = df[[\"mean_loss\", \"mean_acc\", \"config.lrate_sgd\",\"config.lrate_hebb\", \"config.ctx_scaling\",\"config.seed\",\"done\"]]\n",
    "df = df[df[\"done\"]==True]\n",
    "df = df.drop(columns=[\"done\"])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(\"mean_loss\",ascending=True)\n",
    "\n",
    "df.reset_index()\n",
    "print(df.head(15))\n",
    "\n",
    "print(tuner.best_cfg)\n",
    "\n",
    "with open(\"../results/raytune_oja_ctx_interleaved_8episodes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: task a 0.0896, task b -0.3494 | acc: task a 0.5000, task b 0.5000\n",
      "... n_a: 12 n_b: 8\n",
      "step 50, loss: task a -3.0471, task b -2.9381 | acc: task a 0.7500, task b 0.7500\n",
      "... n_a: 2 n_b: 2\n",
      "step 100, loss: task a -7.5096, task b -6.0010 | acc: task a 0.9000, task b 0.8000\n",
      "... n_a: 4 n_b: 2\n",
      "step 150, loss: task a -10.3272, task b -9.0569 | acc: task a 0.9500, task b 0.9500\n",
      "... n_a: 6 n_b: 9\n",
      "step 200, loss: task a -10.3309, task b -9.7294 | acc: task a 0.9000, task b 0.8500\n",
      "... n_a: 11 n_b: 11\n",
      "step 250, loss: task a -13.3375, task b -7.2746 | acc: task a 1.0000, task b 0.7000\n",
      "... n_a: 12 n_b: 22\n",
      "step 300, loss: task a -11.4098, task b -14.2183 | acc: task a 0.9500, task b 1.0000\n",
      "... n_a: 12 n_b: 24\n",
      "step 350, loss: task a -14.5161, task b -13.8416 | acc: task a 1.0000, task b 1.0000\n",
      "... n_a: 12 n_b: 26\n",
      "done\n",
      "config: lrate_sgd: 0.0871, lrate_hebb: 0.0058, context offset: 3\n",
      "terminal accuracy: 1.00, loss: -28.36\n"
     ]
    }
   ],
   "source": [
    "# verify results \n",
    "\n",
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "args.n_episodes = 8\n",
    "args.lrate_hebb = tuner.best_cfg[\"lrate_hebb\"]\n",
    "args.lrate_sgd = tuner.best_cfg[\"lrate_sgd\"]\n",
    "args.ctx_scaling = tuner.best_cfg[\"ctx_scaling\"]\n",
    "args.ctx_avg = False\n",
    "args.training_schedule = \"interleaved\"\n",
    "# np.random.seed(tuner.best_cfg[\"seed\"])\n",
    "# random.seed(tuner.best_cfg[\"seed\"])\n",
    "# torch.manual_seed(tuner.best_cfg[\"seed\"])\n",
    "\n",
    "\n",
    "# create dataset \n",
    "dataset = make_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = MetricLogger(save_dir)\n",
    "model = Nnet(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_model(args, model, optimiser, dataset, logger)\n",
    "\n",
    "print(f\"config: lrate_sgd: {args.lrate_sgd:.4f}, lrate_hebb: {args.lrate_hebb:.4f}, context offset: {args.ctx_scaling}\")\n",
    "print(f\"terminal accuracy: {logger.acc_total[-1]:.2f}, loss: {logger.losses_total[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a3ba60a28f1899318f4810ee01fef19e535f7a46e788980dcac9bebef4b464e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
