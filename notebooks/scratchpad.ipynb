{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad for paper revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import os, sys\n",
    "root_path = os.path.realpath('../')\n",
    "sys.path.append(root_path)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.data import make_dataset\n",
    "from utils.nnet import get_device\n",
    "\n",
    "from hebbcl.logger import MetricLogger\n",
    "from hebbcl.model import Nnet\n",
    "from hebbcl.trainer import Optimiser, train_model\n",
    "from hebbcl.parameters import parser\n",
    "from hebbcl.tuner import HPOTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centering': True,\n",
       " 'ctx_avg': True,\n",
       " 'ctx_avg_alpha': 1,\n",
       " 'ctx_avg_type': 'ema',\n",
       " 'ctx_avg_window': 50,\n",
       " 'ctx_scaling': 2,\n",
       " 'ctx_w_init': 0.5,\n",
       " 'ctx_weights': False,\n",
       " 'cuda': False,\n",
       " 'device': device(type='cpu'),\n",
       " 'gating': 'oja_ctx',\n",
       " 'hebb_normaliser': 10.0,\n",
       " 'log_interval': 50,\n",
       " 'loss_funct': 'reward',\n",
       " 'lrate_hebb': 0.01,\n",
       " 'lrate_sgd': 0.01,\n",
       " 'n_episodes': 200,\n",
       " 'n_features': 27,\n",
       " 'n_hidden': 100,\n",
       " 'n_out': 1,\n",
       " 'n_runs': 50,\n",
       " 'perform_hebb': True,\n",
       " 'perform_sgd': True,\n",
       " 'save_dir': 'simu1',\n",
       " 'save_results': True,\n",
       " 'training_schedule': 'blocked',\n",
       " 'verbose': True,\n",
       " 'weight_init': 0.01}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "dict(sorted(vars(args).items(),key=lambda k: k[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter optimisation\n",
    "I've changed the oja update to be applied to all hidden units. Now networks no longer converge.\n",
    "Let's run HPO to find best-fitting lrate etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centering': True,\n",
       " 'ctx_avg': True,\n",
       " 'ctx_avg_alpha': 1,\n",
       " 'ctx_avg_type': 'ema',\n",
       " 'ctx_avg_window': 50,\n",
       " 'ctx_scaling': 2,\n",
       " 'ctx_w_init': 0.5,\n",
       " 'ctx_weights': False,\n",
       " 'cuda': False,\n",
       " 'gating': 'oja',\n",
       " 'hebb_normaliser': 10.0,\n",
       " 'log_interval': 50,\n",
       " 'loss_funct': 'reward',\n",
       " 'lrate_hebb': 0.01,\n",
       " 'lrate_sgd': 0.01,\n",
       " 'n_episodes': 200,\n",
       " 'n_features': 27,\n",
       " 'n_hidden': 100,\n",
       " 'n_out': 1,\n",
       " 'n_runs': 50,\n",
       " 'perform_hebb': True,\n",
       " 'perform_sgd': True,\n",
       " 'save_dir': 'simu1',\n",
       " 'save_results': True,\n",
       " 'training_schedule': 'blocked',\n",
       " 'verbose': True,\n",
       " 'weight_init': 0.01}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.gating = \"oja\"\n",
    "dict(sorted(vars(args).items(),key=lambda k: k[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tuner\n",
    "tuner = HPOTuner(args, time_budget=60*30, metric=\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPO on blocked with oja (all units)\n",
    "tuner.tune(n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, 29643_00000 to 29643_00499\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   mean_loss                 500 non-null    float64\n",
      " 1   mean_acc                  500 non-null    float64\n",
      " 2   time_this_iter_s          500 non-null    float64\n",
      " 3   done                      500 non-null    bool   \n",
      " 4   timesteps_total           0 non-null      object \n",
      " 5   episodes_total            0 non-null      object \n",
      " 6   training_iteration        500 non-null    int64  \n",
      " 7   neg_mean_loss             500 non-null    float64\n",
      " 8   experiment_id             500 non-null    object \n",
      " 9   date                      500 non-null    object \n",
      " 10  timestamp                 500 non-null    int64  \n",
      " 11  time_total_s              500 non-null    float64\n",
      " 12  pid                       500 non-null    int64  \n",
      " 13  hostname                  500 non-null    object \n",
      " 14  node_ip                   500 non-null    object \n",
      " 15  time_since_restore        500 non-null    float64\n",
      " 16  timesteps_since_restore   500 non-null    int64  \n",
      " 17  iterations_since_restore  500 non-null    int64  \n",
      " 18  experiment_tag            500 non-null    object \n",
      " 19  config.lrate_sgd          500 non-null    float64\n",
      " 20  config.lrate_hebb         500 non-null    float64\n",
      " 21  config.ctx_scaling        500 non-null    int64  \n",
      "dtypes: bool(1), float64(8), int64(6), object(7)\n",
      "memory usage: 86.4+ KB\n"
     ]
    }
   ],
   "source": [
    "tuner.results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>config.lrate_sgd</th>\n",
       "      <th>config.lrate_hebb</th>\n",
       "      <th>config.ctx_scaling</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29643_00246</th>\n",
       "      <td>-29.047512</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.059468</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00158</th>\n",
       "      <td>-29.011238</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.044441</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00010</th>\n",
       "      <td>-28.901655</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00397</th>\n",
       "      <td>-28.890070</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00211</th>\n",
       "      <td>-28.881416</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00361</th>\n",
       "      <td>-28.840702</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.071419</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00387</th>\n",
       "      <td>-28.756092</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00373</th>\n",
       "      <td>-28.750441</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00041</th>\n",
       "      <td>-28.718187</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00248</th>\n",
       "      <td>-28.703583</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.040964</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00369</th>\n",
       "      <td>-28.681864</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00057</th>\n",
       "      <td>-28.665766</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.044220</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00237</th>\n",
       "      <td>-28.565617</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.006078</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00039</th>\n",
       "      <td>-28.479719</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643_00155</th>\n",
       "      <td>-28.430870</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean_loss  mean_acc  config.lrate_sgd  config.lrate_hebb  \\\n",
       "trial_id                                                                \n",
       "29643_00246 -29.047512     1.000          0.059468           0.000203   \n",
       "29643_00158 -29.011238     0.975          0.044441           0.000212   \n",
       "29643_00010 -28.901655     1.000          0.066424           0.000270   \n",
       "29643_00397 -28.890070     0.975          0.020580           0.000212   \n",
       "29643_00211 -28.881416     0.975          0.031433           0.000173   \n",
       "29643_00361 -28.840702     0.975          0.071419           0.000204   \n",
       "29643_00387 -28.756092     0.975          0.020247           0.000222   \n",
       "29643_00373 -28.750441     1.000          0.014567           0.000306   \n",
       "29643_00041 -28.718187     1.000          0.010595           0.000258   \n",
       "29643_00248 -28.703583     0.975          0.040964           0.000161   \n",
       "29643_00369 -28.681864     0.975          0.009152           0.000289   \n",
       "29643_00057 -28.665766     0.950          0.044220           0.000148   \n",
       "29643_00237 -28.565617     0.975          0.006078           0.000327   \n",
       "29643_00039 -28.479719     0.950          0.018087           0.000179   \n",
       "29643_00155 -28.430870     1.000          0.012351           0.000360   \n",
       "\n",
       "             config.ctx_scaling  \n",
       "trial_id                         \n",
       "29643_00246                   7  \n",
       "29643_00158                   6  \n",
       "29643_00010                   6  \n",
       "29643_00397                   2  \n",
       "29643_00211                   6  \n",
       "29643_00361                   4  \n",
       "29643_00387                   7  \n",
       "29643_00373                   4  \n",
       "29643_00041                   5  \n",
       "29643_00248                   1  \n",
       "29643_00369                   3  \n",
       "29643_00057                   7  \n",
       "29643_00237                   2  \n",
       "29643_00039                   3  \n",
       "29643_00155                   4  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuner.results.sort_values(\"mean_acc\",ascending=False).head(20)\n",
    "df = tuner.results\n",
    "df = df[[\"mean_loss\", \"mean_acc\", \"config.lrate_sgd\",\"config.lrate_hebb\", \"config.ctx_scaling\",\"done\"]]\n",
    "df = df.drop(columns=[\"done\"])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(\"mean_loss\",ascending=True)\n",
    "\n",
    "df.reset_index()\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/raytune_oja_blocked.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: task a -0.0465, task b -0.0787 | acc: task a 0.5000, task b 0.5500\n",
      "... n_a: 1 n_b: 7\n",
      "step 50, loss: task a -4.3483, task b 0.1112 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 1 n_b: 1\n",
      "step 100, loss: task a -8.1179, task b 0.1366 | acc: task a 1.0000, task b 0.4500\n",
      "... n_a: 4 n_b: 0\n",
      "step 150, loss: task a -10.9129, task b 0.0347 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 7 n_b: 0\n",
      "step 200, loss: task a -12.4492, task b 0.0419 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 250, loss: task a -13.2750, task b -0.0152 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 300, loss: task a -13.7418, task b 0.0171 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 350, loss: task a -14.0349, task b 0.0247 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 400, loss: task a -14.2267, task b 0.0083 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 450, loss: task a -14.3590, task b 0.0078 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 500, loss: task a -14.4603, task b 0.0090 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 550, loss: task a -14.5351, task b 0.0043 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 600, loss: task a -14.5936, task b 0.0243 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 650, loss: task a -14.6391, task b 0.0114 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 700, loss: task a -14.6763, task b 0.0153 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 750, loss: task a -14.7085, task b 0.0263 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 800, loss: task a -14.7346, task b 0.0079 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 850, loss: task a -14.7560, task b 0.0174 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 900, loss: task a -14.7750, task b 0.0182 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 950, loss: task a -14.7916, task b 0.0189 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1000, loss: task a -14.8063, task b 0.0222 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1050, loss: task a -14.8190, task b 0.0205 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1100, loss: task a -14.8302, task b 0.0188 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1150, loss: task a -14.8403, task b 0.0196 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1200, loss: task a -14.8492, task b 0.0243 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1250, loss: task a -14.8571, task b 0.0227 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1300, loss: task a -14.8645, task b 0.0239 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1350, loss: task a -14.8711, task b 0.0239 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1400, loss: task a -14.8772, task b 0.0243 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1450, loss: task a -14.8829, task b 0.0231 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1500, loss: task a -14.8881, task b 0.0249 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1550, loss: task a -14.8927, task b 0.0252 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1600, loss: task a -14.8970, task b 0.0254 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1650, loss: task a -14.9011, task b 0.0254 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 4 n_b: 0\n",
      "step 1700, loss: task a -14.9049, task b 0.0255 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 5 n_b: 0\n",
      "step 1750, loss: task a -14.9085, task b 0.0221 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 1800, loss: task a -14.9117, task b 0.0263 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 1850, loss: task a -14.9148, task b 0.0236 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 1900, loss: task a -14.9176, task b 0.0248 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 1950, loss: task a -14.9203, task b 0.0246 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2000, loss: task a -14.9229, task b 0.0257 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2050, loss: task a -14.9254, task b 0.0216 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2100, loss: task a -14.9276, task b 0.0251 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2150, loss: task a -14.9297, task b 0.0237 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2200, loss: task a -14.9317, task b 0.0245 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2250, loss: task a -14.9336, task b 0.0236 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2300, loss: task a -14.9354, task b 0.0235 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2350, loss: task a -14.9372, task b 0.0243 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2400, loss: task a -14.9388, task b 0.0241 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2450, loss: task a -14.9404, task b 0.0234 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2500, loss: task a -14.9419, task b 0.0230 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2550, loss: task a -14.9433, task b 0.0221 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2600, loss: task a -14.9447, task b 0.0231 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2650, loss: task a -14.9460, task b 0.0221 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2700, loss: task a -14.9472, task b 0.0228 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2750, loss: task a -14.9484, task b 0.0227 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2800, loss: task a -14.9496, task b 0.0203 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2850, loss: task a -14.9506, task b 0.0228 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2900, loss: task a -14.9517, task b 0.0227 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 2950, loss: task a -14.9527, task b 0.0227 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3000, loss: task a -14.9537, task b 0.0226 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3050, loss: task a -14.9546, task b 0.0226 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3100, loss: task a -14.9555, task b 0.0208 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3150, loss: task a -14.9564, task b 0.0218 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3200, loss: task a -14.9572, task b 0.0225 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3250, loss: task a -14.9580, task b 0.0225 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3300, loss: task a -14.9588, task b 0.0229 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3350, loss: task a -14.9596, task b 0.0223 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3400, loss: task a -14.9603, task b 0.0222 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3450, loss: task a -14.9610, task b 0.0218 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3500, loss: task a -14.9617, task b 0.0222 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3550, loss: task a -14.9624, task b 0.0237 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3600, loss: task a -14.9630, task b 0.0223 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3650, loss: task a -14.9636, task b 0.0226 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3700, loss: task a -14.9642, task b 0.0226 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3750, loss: task a -14.9648, task b 0.0229 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3800, loss: task a -14.9653, task b 0.0236 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3850, loss: task a -14.9659, task b 0.0237 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3900, loss: task a -14.9664, task b 0.0234 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 3950, loss: task a -14.9669, task b 0.0234 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4000, loss: task a -14.9674, task b 0.0223 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4050, loss: task a -14.9679, task b 0.0235 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4100, loss: task a -14.9684, task b 0.0225 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4150, loss: task a -14.9689, task b 0.0240 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4200, loss: task a -14.9693, task b 0.0239 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4250, loss: task a -14.9697, task b 0.0243 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4300, loss: task a -14.9702, task b 0.0228 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4350, loss: task a -14.9706, task b 0.0255 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4400, loss: task a -14.9710, task b 0.0240 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4450, loss: task a -14.9714, task b 0.0241 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4500, loss: task a -14.9717, task b 0.0242 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4550, loss: task a -14.9721, task b 0.0240 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4600, loss: task a -14.9725, task b 0.0241 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4650, loss: task a -14.9728, task b 0.0244 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4700, loss: task a -14.9732, task b 0.0245 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4750, loss: task a -14.9735, task b 0.0245 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4800, loss: task a -14.9739, task b 0.0232 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 7 n_b: 0\n",
      "step 4850, loss: task a -14.9742, task b 0.0245 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 6 n_b: 0\n",
      "step 4900, loss: task a -14.9745, task b 0.0244 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 7 n_b: 0\n",
      "step 4950, loss: task a -14.9748, task b 0.0249 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 7 n_b: 0\n",
      "step 5000, loss: task a -14.9752, task b 0.0059 | acc: task a 1.0000, task b 0.5000\n",
      "... n_a: 7 n_b: 0\n",
      "step 5050, loss: task a -14.9673, task b -1.4701 | acc: task a 1.0000, task b 0.6000\n",
      "... n_a: 2 n_b: 0\n",
      "step 5100, loss: task a -14.9449, task b -2.2096 | acc: task a 1.0000, task b 0.6000\n",
      "... n_a: 1 n_b: 0\n",
      "step 5150, loss: task a -14.9072, task b -2.6769 | acc: task a 1.0000, task b 0.6000\n",
      "... n_a: 1 n_b: 0\n",
      "step 5200, loss: task a -14.8025, task b -3.2885 | acc: task a 1.0000, task b 0.6000\n",
      "... n_a: 1 n_b: 0\n",
      "step 5250, loss: task a -14.4187, task b -4.4996 | acc: task a 1.0000, task b 0.6000\n",
      "... n_a: 1 n_b: 0\n",
      "step 5300, loss: task a -13.1736, task b -6.9967 | acc: task a 0.9000, task b 0.7000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5350, loss: task a -11.3800, task b -9.7849 | acc: task a 0.9000, task b 0.8000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5400, loss: task a -8.9580, task b -11.8354 | acc: task a 0.7000, task b 0.9000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5450, loss: task a -7.5616, task b -12.8243 | acc: task a 0.7000, task b 0.9000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5500, loss: task a -6.6318, task b -13.5486 | acc: task a 0.7000, task b 0.9500\n",
      "... n_a: 0 n_b: 0\n",
      "step 5550, loss: task a -5.6867, task b -14.0968 | acc: task a 0.7000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5600, loss: task a -4.9679, task b -14.3922 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5650, loss: task a -4.5107, task b -14.5469 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5700, loss: task a -4.2048, task b -14.6391 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5750, loss: task a -3.9872, task b -14.7014 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5800, loss: task a -3.8289, task b -14.7449 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5850, loss: task a -3.7044, task b -14.7781 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5900, loss: task a -3.6061, task b -14.8033 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 5950, loss: task a -3.5302, task b -14.8232 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 0\n",
      "step 6000, loss: task a -3.4655, task b -14.8396 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 1\n",
      "step 6050, loss: task a -3.4190, task b -14.8521 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 1\n",
      "step 6100, loss: task a -3.3766, task b -14.8632 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 2\n",
      "step 6150, loss: task a -3.3402, task b -14.8728 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6200, loss: task a -3.3058, task b -14.8816 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6250, loss: task a -3.2773, task b -14.8886 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6300, loss: task a -3.2463, task b -14.8960 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6350, loss: task a -3.2256, task b -14.9012 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6400, loss: task a -3.2024, task b -14.9064 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 2\n",
      "step 6450, loss: task a -3.1814, task b -14.9112 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 2\n",
      "step 6500, loss: task a -3.1618, task b -14.9155 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6550, loss: task a -3.1433, task b -14.9195 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 2\n",
      "step 6600, loss: task a -3.1259, task b -14.9232 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 2\n",
      "step 6650, loss: task a -3.1095, task b -14.9265 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 3\n",
      "step 6700, loss: task a -3.0939, task b -14.9295 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 6750, loss: task a -3.0789, task b -14.9323 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 6800, loss: task a -3.0643, task b -14.9351 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 6850, loss: task a -3.0515, task b -14.9374 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 6900, loss: task a -3.0386, task b -14.9397 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 6950, loss: task a -3.0265, task b -14.9418 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 4\n",
      "step 7000, loss: task a -3.0142, task b -14.9438 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 8\n",
      "step 7050, loss: task a -3.0035, task b -14.9456 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 8\n",
      "step 7100, loss: task a -2.9920, task b -14.9474 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 8\n",
      "step 7150, loss: task a -2.9822, task b -14.9490 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 8\n",
      "step 7200, loss: task a -2.9714, task b -14.9506 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 9\n",
      "step 7250, loss: task a -2.9621, task b -14.9520 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 9\n",
      "step 7300, loss: task a -2.9528, task b -14.9533 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 9\n",
      "step 7350, loss: task a -2.9434, task b -14.9547 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 9\n",
      "step 7400, loss: task a -2.9347, task b -14.9559 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 9\n",
      "step 7450, loss: task a -2.9246, task b -14.9572 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 10\n",
      "step 7500, loss: task a -2.9179, task b -14.9582 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 12\n",
      "step 7550, loss: task a -2.9095, task b -14.9592 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 12\n",
      "step 7600, loss: task a -2.9002, task b -14.9604 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 13\n",
      "step 7650, loss: task a -2.8936, task b -14.9612 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 13\n",
      "step 7700, loss: task a -2.8859, task b -14.9622 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 14\n",
      "step 7750, loss: task a -2.8786, task b -14.9630 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 16\n",
      "step 7800, loss: task a -2.8711, task b -14.9639 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 16\n",
      "step 7850, loss: task a -2.8641, task b -14.9647 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 7900, loss: task a -2.8571, task b -14.9655 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 7950, loss: task a -2.8503, task b -14.9662 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 8000, loss: task a -2.8437, task b -14.9669 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 8050, loss: task a -2.8372, task b -14.9676 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 8100, loss: task a -2.8308, task b -14.9683 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 17\n",
      "step 8150, loss: task a -2.8246, task b -14.9689 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 18\n",
      "step 8200, loss: task a -2.8185, task b -14.9695 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 18\n",
      "step 8250, loss: task a -2.8113, task b -14.9702 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 18\n",
      "step 8300, loss: task a -2.8063, task b -14.9707 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 19\n",
      "step 8350, loss: task a -2.8009, task b -14.9712 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 20\n",
      "step 8400, loss: task a -2.7952, task b -14.9717 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 20\n",
      "step 8450, loss: task a -2.7896, task b -14.9722 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 20\n",
      "step 8500, loss: task a -2.7842, task b -14.9727 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 20\n",
      "step 8550, loss: task a -2.7788, task b -14.9732 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8600, loss: task a -2.7738, task b -14.9736 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8650, loss: task a -2.7685, task b -14.9741 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8700, loss: task a -2.7636, task b -14.9745 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8750, loss: task a -2.7585, task b -14.9749 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8800, loss: task a -2.7532, task b -14.9753 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8850, loss: task a -2.7484, task b -14.9757 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8900, loss: task a -2.7440, task b -14.9761 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 8950, loss: task a -2.7393, task b -14.9765 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9000, loss: task a -2.7348, task b -14.9768 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9050, loss: task a -2.7304, task b -14.9771 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9100, loss: task a -2.7258, task b -14.9775 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9150, loss: task a -2.7213, task b -14.9778 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9200, loss: task a -2.7166, task b -14.9782 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9250, loss: task a -2.7127, task b -14.9784 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9300, loss: task a -2.7088, task b -14.9787 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9350, loss: task a -2.7047, task b -14.9790 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9400, loss: task a -2.7007, task b -14.9793 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9450, loss: task a -2.6969, task b -14.9796 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9500, loss: task a -2.6930, task b -14.9798 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9550, loss: task a -2.6891, task b -14.9801 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9600, loss: task a -2.6853, task b -14.9804 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9650, loss: task a -2.6807, task b -14.9806 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9700, loss: task a -2.6771, task b -14.9809 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9750, loss: task a -2.6741, task b -14.9811 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9800, loss: task a -2.6704, task b -14.9813 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9850, loss: task a -2.6669, task b -14.9816 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9900, loss: task a -2.6632, task b -14.9818 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "step 9950, loss: task a -2.6594, task b -14.9820 | acc: task a 0.6000, task b 1.0000\n",
      "... n_a: 0 n_b: 21\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "dict(sorted(vars(args).items(),key=lambda k: k[0]))\n",
    "\n",
    "# override defaults \n",
    "args.gating = \"oja\"\n",
    "args.perform_hebb = False\n",
    "args.centering = True\n",
    "args.lrate_hebb = 0.000161\n",
    "args.lrate_sgd = 0.040964\n",
    "args.ctx_scaling = 1\n",
    "# create dataset \n",
    "dataset = make_dataset(args)\n",
    "\n",
    "# instantiate logger, model and optimiser:\n",
    "logger = MetricLogger(save_dir)\n",
    "model = Nnet(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_model(args, model, optimiser, dataset, logger)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a3ba60a28f1899318f4810ee01fef19e535f7a46e788980dcac9bebef4b464e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
