{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import os, sys\n",
    "root_path = os.path.realpath('../')\n",
    "sys.path.append(root_path)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from utils.data import make_blobs_dataset\n",
    "from utils.nnet import get_device\n",
    "\n",
    "from utils.nnet import from_gpu\n",
    "from utils.eval import compute_accuracy\n",
    "from hebbcl.logger import MetricLogger\n",
    "from hebbcl.model import ScaledNet2Hidden, ScaledNet2Hidden2Ctx\n",
    "from hebbcl.trainer import Optimiser, train_on_blobs\n",
    "from hebbcl.parameters import parser\n",
    "from hebbcl.tuner import HPOTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: task a -0.0003, task b 0.0021 | acc: task a 0.5000, task b 0.5000\n",
      "step 50, loss: task a -0.1261, task b 0.0173 | acc: task a 0.5000, task b 0.5000\n",
      "step 100, loss: task a -11.7490, task b 1.3865 | acc: task a 1.0000, task b 0.5000\n",
      "step 150, loss: task a -14.9377, task b 0.5166 | acc: task a 1.0000, task b 0.5000\n",
      "step 200, loss: task a -14.9724, task b 0.3387 | acc: task a 1.0000, task b 0.5000\n",
      "step 250, loss: task a -14.9824, task b 0.3281 | acc: task a 1.0000, task b 0.5000\n",
      "step 300, loss: task a -14.9874, task b 0.2937 | acc: task a 1.0000, task b 0.5000\n",
      "step 350, loss: task a -14.9903, task b 0.2793 | acc: task a 1.0000, task b 0.5000\n",
      "step 400, loss: task a -14.9922, task b 0.2493 | acc: task a 1.0000, task b 0.5000\n",
      "step 450, loss: task a -14.9934, task b 0.2332 | acc: task a 1.0000, task b 0.5000\n",
      "step 500, loss: task a -14.9944, task b 0.0589 | acc: task a 1.0000, task b 0.5000\n",
      "step 550, loss: task a -6.4224, task b -13.6189 | acc: task a 0.7000, task b 0.9500\n",
      "step 600, loss: task a -3.7827, task b -14.9889 | acc: task a 0.6500, task b 1.0000\n",
      "step 650, loss: task a -3.6707, task b -14.9950 | acc: task a 0.6000, task b 1.0000\n",
      "step 700, loss: task a -3.6380, task b -14.9964 | acc: task a 0.6000, task b 1.0000\n",
      "step 750, loss: task a -3.6154, task b -14.9972 | acc: task a 0.6000, task b 1.0000\n",
      "step 800, loss: task a -3.5993, task b -14.9977 | acc: task a 0.6000, task b 1.0000\n",
      "step 850, loss: task a -3.5869, task b -14.9981 | acc: task a 0.6000, task b 1.0000\n",
      "step 900, loss: task a -3.5760, task b -14.9983 | acc: task a 0.6000, task b 1.0000\n",
      "step 950, loss: task a -3.5667, task b -14.9985 | acc: task a 0.6000, task b 1.0000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# obtain params\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# set checkpoint directory\n",
    "save_dir = (\n",
    "        Path(\"checkpoints\") / \"test_allhebb\"\n",
    "    ) \n",
    "\n",
    "# get device (gpu/cpu)\n",
    "args.device = get_device(args.cuda)[0]\n",
    "\n",
    "# override defaults \n",
    "# args.n_features = None # todo need to change to flattened size of image\n",
    "args.n_episodes = 20\n",
    "args.lrate_sgd = 0.5\n",
    "args.perform_hebb = False\n",
    "args.ctx_scaling = 2\n",
    "args.ctx_avg = False\n",
    "\n",
    "# create dataset \n",
    "dataset = make_blobs_dataset(args)\n",
    "\n",
    "# instantiate model and optimiser:\n",
    "model = ScaledNet2Hidden2Ctx(args)\n",
    "optimiser = Optimiser(args)\n",
    "\n",
    "# send model to device (GPU?)\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "# train model\n",
    "# send data to gpu\n",
    "x_train = torch.from_numpy(dataset[\"x_train\"]).float().to(args.device)\n",
    "y_train = torch.from_numpy(dataset[\"y_train\"]).float().to(args.device)\n",
    "\n",
    "# test: create test sets\n",
    "x_a = torch.from_numpy(dataset[\"x_task_a\"]).float().to(args.device)\n",
    "r_a = torch.from_numpy(dataset[\"y_task_a\"]).float().to(args.device)\n",
    "\n",
    "x_b = torch.from_numpy(dataset[\"x_task_b\"]).float().to(args.device)\n",
    "r_b = torch.from_numpy(dataset[\"y_task_b\"]).float().to(args.device)\n",
    "\n",
    "x_both = (\n",
    "    torch.from_numpy(np.concatenate((dataset[\"x_task_a\"], dataset[\"x_task_b\"]), axis=0))\n",
    "    .float()\n",
    "    .to(args.device)\n",
    ")\n",
    "r_both = (\n",
    "    torch.from_numpy(np.concatenate((dataset[\"y_task_a\"], dataset[\"y_task_b\"]), axis=0))\n",
    "    .float()\n",
    "    .to(args.device)\n",
    ")\n",
    "\n",
    "f_both = torch.from_numpy(dataset[\"f_all\"]).float().to(args.device)\n",
    "\n",
    "\n",
    "\n",
    "# loop over data and apply optimiser\n",
    "idces = np.arange(len(x_train))\n",
    "for ii, x, y in zip(idces, x_train, y_train):\n",
    "    optimiser.step(model, x, y)\n",
    "    if ii % args.log_interval == 0:            \n",
    "        if args.verbose:\n",
    "            print(\n",
    "                \"step {}, loss: task a {:.4f}, task b {:.4f} | acc: task a {:.4f}, task b {:.4f}\".format(\n",
    "                    str(ii),\n",
    "                    from_gpu(optimiser.loss_funct(r_a, model(x_a))).ravel()[0],\n",
    "                    from_gpu(optimiser.loss_funct(r_b, model(x_b))).ravel()[0],\n",
    "                    compute_accuracy(r_a, model(x_a)),\n",
    "                    compute_accuracy(r_b, model(x_b)),\n",
    "                )\n",
    "            )             \n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data import resize_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"training_data_north_withgarden\", \"training_data_south_withgarden\", \"test_data_north_withgarden\", \"test_data_south_withgarden\"]\n",
    "\n",
    "# for fn in files:\n",
    "#     resize_images(filename=fn, size=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ],\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ],\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ],\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ],\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.69803922, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../datasets/test_data_south_withgarden_ds24.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "data[\"contexts\"]\n",
    "contexts = np.zeros((len(data[\"images\"]),2))\n",
    "contexts[:,data[\"contexts\"]-1] = 1\n",
    "np.concatenate((data[\"images\"].astype(\"float\")/255,contexts),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a3ba60a28f1899318f4810ee01fef19e535f7a46e788980dcac9bebef4b464e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
